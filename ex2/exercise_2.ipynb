{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package framenet_v17 to\n",
      "[nltk_data]     C:\\Users\\lores\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lores\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import framenet as fn\n",
    "from nltk.corpus.reader.wordnet import Synset\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('framenet_v17')\n",
    "nltk.download('wordnet')\n",
    "import hashlib\n",
    "import re\n",
    "from random import randint\n",
    "from random import seed\n",
    "from nltk.corpus import wordnet as wn\n",
    "stop_words = set(stopwords.words('english'))\n",
    "import json\n",
    "from enum import Enum"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the frames from name and surname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frames_IDs():\n",
    "    return [f.ID for f in fn.frames()]   \n",
    "\n",
    "def getFrameSetForStudent(surname, list_len=5):\n",
    "    nof_frames = len(fn.frames())\n",
    "    base_idx = (abs(int(hashlib.sha512(surname.encode('utf-8')).hexdigest(), 16)) % nof_frames)\n",
    "    print('\\nStudent: ' + surname)\n",
    "    framenet_IDs = get_frames_IDs()\n",
    "    i = 0\n",
    "    offset = 0 \n",
    "    seed(1)\n",
    "    frames_extracted = {}\n",
    "    while i < list_len:\n",
    "        fID = framenet_IDs[(base_idx+offset)%nof_frames]\n",
    "        f = fn.frame(fID)\n",
    "        fNAME = f.name\n",
    "        frames_extracted[fID] = fNAME\n",
    "        print('\\tID: {a:4d}\\tFrame: {framename}'.format(a=fID, framename=fNAME))\n",
    "        offset = randint(0, nof_frames)\n",
    "        i += 1\n",
    "    return frames_extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Student: Lorenzo Botto\n",
      "\tID: 1882\tFrame: Shopping\n",
      "\tID: 1148\tFrame: Attributed_information\n",
      "\tID: 2231\tFrame: Response_scenario\n",
      "\tID: 2191\tFrame: Turning_out\n",
      "\tID:  380\tFrame: Custom\n",
      "\n",
      "Student: Gabriele Naretto\n",
      "\tID: 2018\tFrame: Collocation_image_schema\n",
      "\tID: 1497\tFrame: Giving_in\n",
      "\tID:  334\tFrame: Cause_temperature_change\n",
      "\tID: 1211\tFrame: Dressing\n",
      "\tID: 1147\tFrame: Ordinal_numbers\n"
     ]
    }
   ],
   "source": [
    "# I extract the frames for Lorenzo Botto and Gabriele Naretto and I merge them in a single dictionary\n",
    "\n",
    "frames_student1 = getFrameSetForStudent('Lorenzo Botto')\n",
    "frames_student2 = getFrameSetForStudent('Gabriele Naretto')\n",
    "frames_extracted = {\"lorenzo\": frames_student1} | {\"gabriele\": frames_student2}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the support function for cleaning the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I clean the sentences removing the punctuation and the stop words\n",
    "\n",
    "def clean_sentence(sentence: str | list) -> list:\n",
    "    sentence_without_puntuaction = re.sub(r'[^\\w\\s]', '', str(sentence)).strip()\n",
    "    word_tokens = word_tokenize(sentence_without_puntuaction)\n",
    "    filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "    return filtered_sentence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the lesk algoritm for found the best synset in wordnet\n",
    "\n",
    "For every synset (i.e., for each hyperonym and hyponym) we calculate the overlap between the gloss and the sentence and we return the result of the overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that given a list of synsets (hypernyms and hyponyms) returns the value of the maximum synset\n",
    "\n",
    "def search_in_synsets(synsets: list, sentence: str, clean: bool) -> int:\n",
    "    overlap = 0\n",
    "    for synset in synsets:\n",
    "        for definition_word in clean_sentence(synset.definition()) if clean else synset.definition().split():\n",
    "            if definition_word in sentence:\n",
    "                overlap += 1\n",
    "        for example_word in clean_sentence(synset.examples()) if clean else synset.examples():\n",
    "            if clean:\n",
    "                if example_word in sentence:\n",
    "                    overlap += 1\n",
    "            else:\n",
    "                for example_word in example_word.split():\n",
    "                    if example_word in sentence:\n",
    "                        overlap += 1\n",
    "    return overlap\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesk Algorithm\n",
    "\n",
    "We take all the synsets of the word and we calculate the overlap, for each synset of the word, of all hypernyms and hyponyms and between the gloss and definition of the synset. We return the synset with the highest overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lesk(word: str, sentence: list, clean: bool) -> Synset:\n",
    "    synsets = wn.synsets(word)\n",
    "    if not synsets:\n",
    "        return None\n",
    "    best_sense = synsets[0]\n",
    "    max_overlap = 0\n",
    "    for synset in synsets:\n",
    "        overlap = 0\n",
    "        overlap += search_in_synsets(synset.hypernyms(), sentence, clean)\n",
    "        overlap += search_in_synsets(synset.hyponyms(), sentence, clean)\n",
    "        \n",
    "        # Cleaning example in wordnet \n",
    "        for definition_word in clean_sentence(synset.definition()) if clean else synset.definition().split():\n",
    "            if definition_word in sentence:\n",
    "                overlap += 1\n",
    "        for example_word in clean_sentence(synset.examples()) if clean else synset.examples():\n",
    "            if clean:\n",
    "                if example_word in sentence:\n",
    "                    overlap += 1\n",
    "            else:\n",
    "                for example_word in example_word.split():\n",
    "                    if example_word in sentence:\n",
    "                        overlap += 1\n",
    "        if overlap > max_overlap:\n",
    "            max_overlap = overlap\n",
    "            best_sense = synset\n",
    "        \n",
    "    return best_sense"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the cycles for finding the accuracy of the lesk algorithm vs. handwritten annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I read the json file and extract the synsets annotated by hand\n",
    "annotations = json.load(open('dataset/annotations.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I create the enum class to manage the different types of cycles\n",
    "\n",
    "class CycleType(Enum):\n",
    "    FRAME = 0\n",
    "    FRAME_FE = 1\n",
    "    FRAME_FE_LU = 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function for calculating the accuracy\n",
    "\n",
    "We use three types of structures for the sentences:\n",
    "1. The first one (CycleType = FRAME) is a cycle with only frame definitions, frame elements definitions and lexical units definitions.\n",
    "2. The second one (CycleType = FRAME_FE) is a cycle with frame definitions, frame elements definitions (with frames definition) and lexical units definitions (with frames definition).\n",
    "3. The third one (CycleType = FRAME_FE_LU) is a cycle frame definitions (with frame elements definition and lexical units definition), frame elements definitions (with frames definition and lexical units definition) and lexical units definitions (with frame definition and frame elements definition)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I use the function cycle_base for calculate the accuracy, passing only the definitions of frame, \n",
    "# frame elements and lexical units to Lesk algorithm\n",
    "\n",
    "def calculate_accuracy(cycle_type: CycleType) -> float:\n",
    "    points = 0\n",
    "    total = 0\n",
    "    \n",
    "    # For all the student\n",
    "    for student in frames_extracted:\n",
    "        # For all the name of frames we use Lesk algorithm for return the best sense in wordnet\n",
    "        for frames_id in frames_extracted[student]:            \n",
    "            # I extract some information about the frame and the annotation for reusing them multiple times\n",
    "            frame = fn.frame(frames_id)\n",
    "\n",
    "            annotation_frame = annotations[frame.name.lower()]\n",
    "            frame_name = frame.name.split(\"_\")[0] if \"_\" in frame.name else frame.name\n",
    "\n",
    "            # I create the structure of sentences if cycle_type is FRAME_FE_LU\n",
    "            # In every case I call the Lesk algorithm for finding the best sense\n",
    "            match cycle_type:\n",
    "                case CycleType.FRAME_FE_LU:\n",
    "                    list_sentences = [frame.definition]\n",
    "                    for unit in list(frame.FE):\n",
    "                        list_sentences.append(frame.FE[unit].definition)\n",
    "                    for lexical in list(frame.lexUnit):\n",
    "                        list_sentences.append(frame.lexUnit[lexical].definition.replace(\"COD: \", \"\").replace(\"FN: \", \"\"))\n",
    "                    lesk_synset_name = lesk(frame_name, clean_sentence(list_sentences), True).name()\n",
    "                case CycleType.FRAME_FE | CycleType.FRAME:\n",
    "                    lesk_synset_name = lesk(frame_name, clean_sentence(frame.definition), True).name()\n",
    "            \n",
    "            annotation_frame_synset = annotation_frame[\"frame_name\"]\n",
    "            \n",
    "            # I check if the synset returned by Lesk algorithm is the same of the annotation\n",
    "            if lesk_synset_name == annotation_frame_synset:     \n",
    "                points += 1\n",
    "            total += 1\n",
    "            \n",
    "            # For every frame element in the frame we use Lesk algorithm for return the best sense in wordnet\n",
    "            for unit in list(frame.FE):\n",
    "                if unit.lower() in annotation_frame[\"frame_elements\"]:\n",
    "                    frame_element = frame.FE[unit]\n",
    "                    frame_element_name = frame_element.name.split(\"_\")[0] if \"_\" in frame_element.name else frame_element.name\n",
    "                    #print(\"Frame Element Name:\", frame_element_name)\n",
    "                    \n",
    "                    # I compare the different types of cycles for passing the correct parameter (list of sentences) \n",
    "                    # to Lesk algorithm\n",
    "                    match cycle_type:\n",
    "                        case CycleType.FRAME_FE_LU:\n",
    "                            lesk_synset_name = lesk(frame_element_name, clean_sentence(list_sentences), True).name()\n",
    "                        case CycleType.FRAME_FE:\n",
    "                            list_sentences = [frame.definition, frame_element.definition]\n",
    "                            lesk_synset_name = lesk(frame_element_name, clean_sentence(list_sentences), True).name()\n",
    "                        case CycleType.FRAME:\n",
    "                            lesk_synset_name = lesk(frame_element_name, clean_sentence(frame_element.definition), True).name()\n",
    "                    \n",
    "                    annotation_frame_synset = annotation_frame[\"frame_elements\"][unit.lower()]\n",
    "\n",
    "                    # I check if the synset returned by Lesk algorithm is the same of the annotation\n",
    "                    if lesk_synset_name == annotation_frame_synset:\n",
    "                        points += 1\n",
    "                    total += 1\n",
    "            \n",
    "            # For every lexical unit in the frame we use Lesk algorithm for return the best sense in wordnet\n",
    "            for lexical in list(frame.lexUnit):\n",
    "                if frame.lexUnit[lexical].lexemes[0].name.lower() in annotation_frame[\"lexical_units\"]:\n",
    "                    lexical_unit = frame.lexUnit[lexical]\n",
    "                    lexical_unit_lexeme = lexical_unit.lexemes[0].name\n",
    "                    lexical_unit_definition = lexical_unit.definition.replace(\"COD: \", \"\").replace(\"FN: \", \"\")\n",
    "                    #print(\"Lexical Unit Name:\", lexical_unit_lexeme)\n",
    "\n",
    "                    # I compare the different types of cycles for passing the correct parameter (list of sentences) \n",
    "                    # to Lesk algorithm\n",
    "                    match cycle_type:\n",
    "                        case CycleType.FRAME_FE_LU:\n",
    "                            lesk_synset_name = lesk(lexical_unit_lexeme, clean_sentence(list_sentences), True).name()\n",
    "                        case CycleType.FRAME_FE:\n",
    "                            list_sentences = [frame.definition, lexical_unit_definition]\n",
    "                            lesk_synset_name = lesk(lexical_unit_lexeme, clean_sentence(list_sentences), True).name()\n",
    "                        case CycleType.FRAME:\n",
    "                            lesk_synset_name = lesk(lexical_unit_lexeme, clean_sentence(lexical_unit_definition), True).name()\n",
    "                    \n",
    "                    annotation_frame_synset = annotation_frame[\"lexical_units\"][lexical_unit_lexeme.lower()]\n",
    "\n",
    "                    # I check if the synset returned by Lesk algorithm is the same of the annotation\n",
    "                    if lesk_synset_name == annotation_frame_synset:\n",
    "                        points += 1\n",
    "                    total += 1\n",
    "    \n",
    "    return points / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "Cycle with only frame definitions, frame elements definitions and lexical units definitions\n",
      "Accuracy: 0.375886524822695\n",
      "-----------------------------------\n",
      "Cycle with frame definitions (with frame elements definition and lexical units definition), frame elements definitions (with frames definition and lexical units definition) and lexical units definitions (with frame definition and frame elements definition)\n",
      "Accuracy: 0.3475177304964539\n",
      "-----------------------------------\n",
      "Cycle with frame definitions, frame elements definitions (with frames definition) and lexical units definitions (with frames definition)\n",
      "Accuracy: 0.3900709219858156\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"-----------------------------------\")\n",
    "print(\"Cycle with only frame definitions, frame elements definitions and lexical units definitions\")\n",
    "print(\"Accuracy:\", calculate_accuracy(CycleType.FRAME))\n",
    "print(\"-----------------------------------\")\n",
    "print(\"Cycle with frame definitions (with frame elements definition and lexical units definition), frame elements definitions (with frames definition and lexical units definition) and lexical units definitions (with frame definition and frame elements definition)\")\n",
    "print(\"Accuracy:\", calculate_accuracy(CycleType.FRAME_FE_LU))\n",
    "print(\"-----------------------------------\")\n",
    "print(\"Cycle with frame definitions, frame elements definitions (with frames definition) and lexical units definitions (with frames definition)\")\n",
    "print(\"Accuracy:\", calculate_accuracy(CycleType.FRAME_FE))\n",
    "print(\"-----------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
