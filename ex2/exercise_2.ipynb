{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package framenet_v17 to\n",
      "[nltk_data]     C:\\Users\\gabri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\framenet_v17.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\gabri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import framenet as fn\n",
    "from nltk.corpus.reader.wordnet import Synset\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('framenet_v17')\n",
    "nltk.download('wordnet')\n",
    "import hashlib\n",
    "import re\n",
    "from random import randint\n",
    "from random import seed\n",
    "from nltk.corpus import wordnet as wn\n",
    "stop_words = set(stopwords.words('english'))\n",
    "import json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the frames from name and surname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frames_IDs():\n",
    "    return [f.ID for f in fn.frames()]   \n",
    "\n",
    "def getFrameSetForStudent(surname, list_len=5):\n",
    "    nof_frames = len(fn.frames())\n",
    "    base_idx = (abs(int(hashlib.sha512(surname.encode('utf-8')).hexdigest(), 16)) % nof_frames)\n",
    "    print('\\nStudent: ' + surname)\n",
    "    framenet_IDs = get_frames_IDs()\n",
    "    i = 0\n",
    "    offset = 0 \n",
    "    seed(1)\n",
    "    frames_extracted = {}\n",
    "    while i < list_len:\n",
    "        fID = framenet_IDs[(base_idx+offset)%nof_frames]\n",
    "        f = fn.frame(fID)\n",
    "        fNAME = f.name\n",
    "        frames_extracted[fID] = fNAME\n",
    "        print('\\tID: {a:4d}\\tFrame: {framename}'.format(a=fID, framename=fNAME))\n",
    "        offset = randint(0, nof_frames)\n",
    "        i += 1\n",
    "    return frames_extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Student: Lorenzo Botto\n",
      "\tID: 1882\tFrame: Shopping\n",
      "\tID: 1148\tFrame: Attributed_information\n",
      "\tID: 2231\tFrame: Response_scenario\n",
      "\tID: 2191\tFrame: Turning_out\n",
      "\tID:  380\tFrame: Custom\n",
      "\n",
      "Student: Gabriele Naretto\n",
      "\tID: 2018\tFrame: Collocation_image_schema\n",
      "\tID: 1497\tFrame: Giving_in\n",
      "\tID:  334\tFrame: Cause_temperature_change\n",
      "\tID: 1211\tFrame: Dressing\n",
      "\tID: 1147\tFrame: Ordinal_numbers\n"
     ]
    }
   ],
   "source": [
    "# I extract the frames for Lorenzo Botto and Gabriele Naretto and I merge them in a single dictionary\n",
    "\n",
    "frames_student1 = getFrameSetForStudent('Lorenzo Botto')\n",
    "frames_student2 = getFrameSetForStudent('Gabriele Naretto')\n",
    "frames_extracted = {\"lorenzo\": frames_student1} | {\"gabriele\": frames_student2}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the support function for cleaning the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I clean the sentences removing the punctuation and the stop words\n",
    "\n",
    "def clean_sentence(sentence: str | list) -> list:\n",
    "    sentence_without_puntuaction = re.sub(r'[^\\w\\s]', '', str(sentence)).strip()\n",
    "    word_tokens = word_tokenize(sentence_without_puntuaction)\n",
    "    filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "    return filtered_sentence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the lesk algoritm for found the best synset in wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that given a list of synsets (hypernyms and hyponyms) returns the value of the maximum synset\n",
    "\n",
    "def search_in_synsets(synsets: list, sentence: str, clean: bool) -> int:\n",
    "    overlap = 0\n",
    "    for synset in synsets:\n",
    "        for definition_word in clean_sentence(synset.definition()) if clean else synset.definition().split():\n",
    "            if definition_word in sentence:\n",
    "                overlap += 1\n",
    "        for example_word in clean_sentence(synset.examples()) if clean else synset.examples():\n",
    "            if clean:\n",
    "                if example_word in sentence:\n",
    "                    overlap += 1\n",
    "            else:\n",
    "                for example_word in example_word.split():\n",
    "                    if example_word in sentence:\n",
    "                        overlap += 1\n",
    "    return overlap\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lesk(word: str, sentence: list, clean: bool) -> Synset:\n",
    "    synsets = wn.synsets(word)\n",
    "    if not synsets:\n",
    "        return None\n",
    "    best_sense = synsets[0]\n",
    "    max_overlap = 0\n",
    "    for synset in synsets:\n",
    "        overlap = 0\n",
    "        overlap += search_in_synsets(synset.hypernyms(), sentence, clean)\n",
    "        overlap += search_in_synsets(synset.hyponyms(), sentence, clean)\n",
    "        \n",
    "        # Cleaning example in wordnet \n",
    "        for definition_word in clean_sentence(synset.definition()) if clean else synset.definition().split():\n",
    "            if definition_word in sentence:\n",
    "                overlap += 1\n",
    "        for example_word in clean_sentence(synset.examples()) if clean else synset.examples():\n",
    "            if clean:\n",
    "                if example_word in sentence:\n",
    "                    overlap += 1\n",
    "            else:\n",
    "                for example_word in example_word.split():\n",
    "                    if example_word in sentence:\n",
    "                        overlap += 1\n",
    "        if overlap > max_overlap:\n",
    "            max_overlap = overlap\n",
    "            best_sense = synset\n",
    "        \n",
    "    return best_sense"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the cycle for finding the accuracy of the lesk algorithm vs. handwritten annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "Frame name: Shopping\n",
      "Annotation: shopping.n.01\n",
      "Lesk synset: shopping.n.01\n",
      "-----------------------------------\n",
      "Frame name: Attributed_information\n",
      "Annotation: impute.v.01\n",
      "Lesk synset: impute.v.01\n",
      "-----------------------------------\n",
      "Frame name: Response_scenario\n",
      "Annotation: reaction.n.03\n",
      "Lesk synset: response.n.01\n",
      "-----------------------------------\n",
      "Frame name: Turning_out\n",
      "Annotation: turn.v.19\n",
      "Lesk synset: turn.v.01\n",
      "-----------------------------------\n",
      "Frame name: Custom\n",
      "Annotation: custom.n.01\n",
      "Lesk synset: custom.n.01\n",
      "-----------------------------------\n",
      "Accuracy: 0.6\n"
     ]
    }
   ],
   "source": [
    "# I read the json file and extract the synsets annotated by hand\n",
    "annotations = json.load(open('dataset/annotations.json'))\n",
    "points = 0\n",
    "\n",
    "# For all the name of frames we use Lesk algorithm for return the best sense in wordnet\n",
    "for frames_id in frames_extracted[\"lorenzo\"]:\n",
    "    print(\"-----------------------------------\")\n",
    "    print(\"Frame name:\", fn.frame(frames_id).name)\n",
    "    \n",
    "    # I check the synset of the frame name\n",
    "    if \"_\" in fn.frame(frames_id).name:\n",
    "        if lesk(fn.frame(frames_id).name.split(\"_\")[0], clean_sentence(fn.frame(frames_id).definition), True).name() == annotations[fn.frame(frames_id).name.lower()][\"frame_name\"]:     \n",
    "            points += 1\n",
    "        print(\"Annotation:\", annotations[fn.frame(frames_id).name.lower()][\"frame_name\"])\n",
    "        print(\"Lesk synset:\", lesk(fn.frame(frames_id).name.split(\"_\")[0], clean_sentence(fn.frame(frames_id).definition), True).name())\n",
    "    else:\n",
    "        if lesk(fn.frame(frames_id).name, clean_sentence(fn.frame(frames_id).definition), True).name() == annotations[fn.frame(frames_id).name.lower()][\"frame_name\"]:\n",
    "            points += 1\n",
    "        print(\"Annotation:\", annotations[fn.frame(frames_id).name.lower()][\"frame_name\"])\n",
    "        print(\"Lesk synset:\", lesk(fn.frame(frames_id).name.split(\"_\")[0], clean_sentence(fn.frame(frames_id).definition), True).name())\n",
    "\n",
    "    # TODO: I check the synset of the frame elements and the lexical units\n",
    "    # print(\"Frame Elements in the frame:\", list(fn.frame(frames_id).FE))\n",
    "    # for unit in list(fn.frame(frames_id).FE):\n",
    "    #     print(lesk(unit,fn.frame(frames_id).FE[unit].definition,True))\n",
    "    \n",
    "    # print(\"Lexical Units in the frame:\", list(fn.frame(frames_id).lexUnit))\n",
    "    # #for every lexical unit in the frame we use lesk algoritm for return the best sense in wordnet\n",
    "    # for lexical in list(fn.frame(frames_id).lexUnit):\n",
    "    #     print(lesk(fn.frame(frames_id).lexUnit[lexical].lexemes[0].name,fn.frame(frames_id).lexUnit[lexical].definition,True))\n",
    "    \n",
    "print(\"-----------------------------------\")\n",
    "print(\"Accuracy:\", points / len(frames_extracted[\"lorenzo\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
