{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.wsd import lesk\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/TLN-definitions-23.tsv', sep='\\t')\n",
    "df_door = df['door']\n",
    "df_ladybug = df['ladybug']\n",
    "df_pain = df['pain']\n",
    "df_blurriness = df['blurriness']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing of definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop words removal\n",
    "df_door_nostop = df_door.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "df_ladybug_nostop = df_ladybug.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "df_pain_nostop = df_pain.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "df_blurriness_nostop = df_blurriness.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "\n",
    "# Lemmatization and tokenization (with puntuaction removal)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df_door_lem = df_door_nostop.apply(\n",
    "    lambda x: tokenizer.tokenize(' '.join([lemmatizer.lemmatize(word) for word in x.split()]).lower())\n",
    ")\n",
    "df_ladybug_lem = df_ladybug_nostop.apply(\n",
    "    lambda x: tokenizer.tokenize(' '.join([lemmatizer.lemmatize(word) for word in x.split()]).lower())\n",
    ")\n",
    "df_pain_lem = df_pain_nostop.apply(\n",
    "    lambda x: tokenizer.tokenize(' '.join([lemmatizer.lemmatize(word) for word in x.split()]).lower())\n",
    ")\n",
    "df_blurriness_lem = df_blurriness_nostop.apply(\n",
    "    lambda x: tokenizer.tokenize(' '.join([lemmatizer.lemmatize(word) for word in x.split()]).lower())\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the words with highest frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I take the words with most frequency\n",
    "def get_frequent_words(df):\n",
    "    words = []\n",
    "    for row in df:\n",
    "        for word in row:\n",
    "            if len(word) > 2:\n",
    "                words.append(word)\n",
    "    return pd.Series(words).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_door = get_frequent_words(df_door_lem)[:20]\n",
    "words_ladybug = get_frequent_words(df_ladybug_lem)[:20]\n",
    "words_pain = get_frequent_words(df_pain_lem)[:20]\n",
    "words_blurriness = get_frequent_words(df_blurriness_lem)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_best_synset(words: pd.Series, df: pd.DataFrame):\n",
    "    # I take the hypernyms of the words with most frequency\n",
    "    hypernyms = []\n",
    "    for word in words.index:\n",
    "        synset = lesk(df.to_list(), word, 'n')\n",
    "        if synset:\n",
    "            hypernyms.extend(synset.hypernyms())\n",
    "\n",
    "    hypernyms = set(hypernyms)\n",
    "\n",
    "    results = []\n",
    "    for hyp in hypernyms:\n",
    "        # I take the definition and examples of the hypernyms\n",
    "        hyp_def_examples = hyp.definition() + ', '.join(hyp.examples())\n",
    "        \n",
    "        # I found the words that are in mine top words and in the definition of the hypernyms\n",
    "        # I associate a score to the hypernyms based on the frequency of the top words founded in the definition\n",
    "        score = 0\n",
    "        matched_words = []\n",
    "        for word in words.index:\n",
    "            if word in hyp_def_examples:\n",
    "                matched_words.append(word)\n",
    "                score += words[word]\n",
    "        if score > 0:\n",
    "            results.append((hyp, matched_words, score))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "Door synsets:\n",
      "Synset: Synset('environment.n.01')\n",
      "Matched words: ['room']\n",
      "Score: 13\n",
      "Synset: Synset('gathering.n.01')\n",
      "Matched words: ['one', 'place']\n",
      "Score: 6\n",
      "Synset: Synset('unit.n.04')\n",
      "Matched words: ['another', 'one']\n",
      "Score: 6\n",
      "Synset: Synset('wrestling_hold.n.01')\n",
      "Matched words: ['used']\n",
      "Score: 5\n",
      "Synset: Synset('commercial_enterprise.n.02')\n",
      "Matched words: ['used']\n",
      "Score: 5\n",
      "-----------------------------------\n",
      "Ladybug synsets:\n",
      "Synset: Synset('disk.n.01')\n",
      "Matched words: ['round']\n",
      "Score: 4\n",
      "-----------------------------------\n",
      "Pain synsets:\n",
      "Synset: Synset('suffering.n.04')\n",
      "Matched words: ['feeling', 'physical', 'pain']\n",
      "Score: 30\n",
      "Synset: Synset('pain.n.02')\n",
      "Matched words: ['feeling', 'emotional', 'pain', 'emotion']\n",
      "Score: 29\n",
      "Synset: Synset('feeling.n.01')\n",
      "Matched words: ['feeling', 'emotional', 'emotion']\n",
      "Score: 27\n",
      "Synset: Synset('quality.n.01')\n",
      "Matched words: ['something']\n",
      "Score: 4\n",
      "Synset: Synset('fabric.n.01')\n",
      "Matched words: ['felt']\n",
      "Score: 3\n",
      "-----------------------------------\n",
      "Blurriness synsets:\n",
      "Synset: Synset('edge.n.06')\n",
      "Matched words: ['something', 'object']\n",
      "Score: 9\n",
      "Synset: Synset('need.n.01')\n",
      "Matched words: ['condition']\n",
      "Score: 5\n",
      "Synset: Synset('quality.n.01')\n",
      "Matched words: ['something']\n",
      "Score: 5\n",
      "Synset: Synset('hole.n.02')\n",
      "Matched words: ['something']\n",
      "Score: 5\n",
      "Synset: Synset('rhetorical_device.n.01')\n",
      "Matched words: ['effect']\n",
      "Score: 4\n"
     ]
    }
   ],
   "source": [
    "# Maintain only the 5 best synset with highest score\n",
    "def get_best_synset(results):\n",
    "    results.sort(key=lambda x: x[2], reverse=True)\n",
    "    return results[:5]\n",
    "\n",
    "\n",
    "door_synsets = get_best_synset(search_best_synset(words_door, df_door))\n",
    "ladybug_synsets = get_best_synset(search_best_synset(words_ladybug, df_ladybug))\n",
    "pain_synsets = get_best_synset(search_best_synset(words_pain, df_pain))\n",
    "bluriness_synsets = get_best_synset(search_best_synset(words_blurriness, df_blurriness))\n",
    "\n",
    "print(\"-----------------------------------\")\n",
    "print('Door synsets:')\n",
    "for synset in door_synsets:\n",
    "    print('Synset:', synset[0])\n",
    "    print('Matched words:', synset[1])\n",
    "    print('Score:', synset[2])\n",
    "\n",
    "print(\"-----------------------------------\")\n",
    "print('Ladybug synsets:')\n",
    "for synset in ladybug_synsets:\n",
    "    print('Synset:', synset[0])\n",
    "    print('Matched words:', synset[1])\n",
    "    print('Score:', synset[2])\n",
    "\n",
    "print(\"-----------------------------------\")\n",
    "print('Pain synsets:')\n",
    "for synset in pain_synsets:\n",
    "    print('Synset:', synset[0])\n",
    "    print('Matched words:', synset[1])\n",
    "    print('Score:', synset[2])\n",
    "\n",
    "print(\"-----------------------------------\")\n",
    "print('Blurriness synsets:')\n",
    "for synset in bluriness_synsets:\n",
    "    print('Synset:', synset[0])\n",
    "    print('Matched words:', synset[1])\n",
    "    print('Score:', synset[2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
