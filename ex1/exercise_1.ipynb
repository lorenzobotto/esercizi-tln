{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from statistics import mean\n",
    "from IPython.display import display, HTML\n",
    "from nltk import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.spatial import distance\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/TLN-definitions-23.tsv', sep='\\t')\n",
    "df_door = df['door']\n",
    "df_ladybug = df['ladybug']\n",
    "df_pain = df['pain']\n",
    "df_blurriness = df['blurriness']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop words removal\n",
    "df_door_nostop = df_door.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "df_ladybug_nostop = df_ladybug.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "df_pain_nostop = df_pain.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "df_blurriness_nostop = df_blurriness.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "\n",
    "# Stemming and tokenization (with puntuaction removal)\n",
    "ps = PorterStemmer()\n",
    "df_door_stem = df_door_nostop.apply(\n",
    "    lambda x: tokenizer.tokenize(' '.join([ps.stem(word) for word in x.split()]).lower())\n",
    ")\n",
    "df_ladybug_stem = df_ladybug_nostop.apply(\n",
    "    lambda x: tokenizer.tokenize(' '.join([ps.stem(word) for word in x.split()]).lower())\n",
    ")\n",
    "df_pain_stem = df_pain_nostop.apply(\n",
    "    lambda x: tokenizer.tokenize(' '.join([ps.stem(word) for word in x.split()]).lower())\n",
    ")\n",
    "df_blurriness_stem = df_blurriness_nostop.apply(\n",
    "    lambda x: tokenizer.tokenize(' '.join([ps.stem(word) for word in x.split()]).lower())\n",
    ")\n",
    "\n",
    "# Lemmatization and tokenization (with puntuaction removal)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df_door_lem = df_door_nostop.apply(\n",
    "    lambda x: tokenizer.tokenize(' '.join([lemmatizer.lemmatize(word) for word in x.split()]).lower())\n",
    ")\n",
    "df_ladybug_lem = df_ladybug_nostop.apply(\n",
    "    lambda x: tokenizer.tokenize(' '.join([lemmatizer.lemmatize(word) for word in x.split()]).lower())\n",
    ")\n",
    "df_pain_lem = df_pain_nostop.apply(\n",
    "    lambda x: tokenizer.tokenize(' '.join([lemmatizer.lemmatize(word) for word in x.split()]).lower())\n",
    ")\n",
    "df_blurriness_lem = df_blurriness_nostop.apply(\n",
    "    lambda x: tokenizer.tokenize(' '.join([lemmatizer.lemmatize(word) for word in x.split()]).lower())\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Method: Overlap of words in the definitions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for calculating the overlap between the definitions of two words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_overlap(sentence_1: list, sentence_2: list) -> int:\n",
    "    return len(set(sentence_1).intersection(set(sentence_2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def definitions_similarity(definitions: list) -> float:\n",
    "    similarity_matrix = []\n",
    "    for row in definitions:\n",
    "        for row2 in definitions:\n",
    "            if row == row2:\n",
    "                continue\n",
    "            overlap = calculate_overlap(row, row2)\n",
    "            overlap = overlap / (len(row) if len(row) < len(row2) else len(row2))\n",
    "            similarity_matrix.append(overlap)\n",
    "    return round(mean(similarity_matrix), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between all definitions with Stemming\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Concrete</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Generic</th>\n",
       "      <td>pain<br> 0.22</td>\n",
       "      <td>door<br> 0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Specific</th>\n",
       "      <td>bluriness<br> 0.08</td>\n",
       "      <td>ladybug<br> 0.56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between all definitions with Lemmatization\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Concrete</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Generic</th>\n",
       "      <td>pain<br> 0.20</td>\n",
       "      <td>door<br> 0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Specific</th>\n",
       "      <td>bluriness<br> 0.08</td>\n",
       "      <td>ladybug<br> 0.53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Overlap between all definitions of door, ladybug, pain and blurriness with both stemming and lemmatization\n",
    "door_similarity_stem = definitions_similarity(df_door_stem)\n",
    "ladybug_similarity_stem = definitions_similarity(df_ladybug_stem)\n",
    "pain_similarity_stem = definitions_similarity(df_pain_stem)\n",
    "blurriness_similarity_stem = definitions_similarity(df_blurriness_stem)\n",
    "\n",
    "door_similarity_lem = definitions_similarity(df_door_lem)\n",
    "ladybug_similarity_lem = definitions_similarity(df_ladybug_lem)\n",
    "pain_similarity_lem = definitions_similarity(df_pain_lem)\n",
    "blurriness_similarity_lem = definitions_similarity(df_blurriness_lem)\n",
    "\n",
    "# I create the dataframe for printing the table with the results of the similarity with stemming\n",
    "\n",
    "data_stem =   [['pain\\n {:.2f}'.format(pain_similarity_stem), 'door\\n {:.2f}'.format(door_similarity_stem)], \n",
    "             ['bluriness\\n {:.2f}'.format(blurriness_similarity_stem), 'ladybug\\n {:.2f}'.format(ladybug_similarity_stem)]]\n",
    "df_similarity_stem = pd.DataFrame(data_stem, columns=['Abstract', 'Concrete'], index=['Generic', 'Specific'])\n",
    "\n",
    "print(\"Similarity between all definitions with Stemming\")\n",
    "display(HTML(df_similarity_stem.to_html().replace(\"\\\\n\",\"<br>\")))\n",
    "\n",
    "# I create the dataframe for printing the table with the results of the similarity with lemmatization\n",
    "\n",
    "data_lem =  [['pain\\n {:.2f}'.format(pain_similarity_lem), 'door\\n {:.2f}'.format(door_similarity_lem)],\n",
    "                ['bluriness\\n {:.2f}'.format(blurriness_similarity_lem), 'ladybug\\n {:.2f}'.format(ladybug_similarity_lem)]]\n",
    "df_similarity_lem = pd.DataFrame(data_lem, columns=['Abstract', 'Concrete'], index=['Generic', 'Specific'])\n",
    "print(\"Similarity between all definitions with Lemmatization\")\n",
    "display(HTML(df_similarity_lem.to_html().replace(\"\\\\n\",\"<br>\")))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Method: Similarity of definitions with CountVectorizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "tdidf_door_lem = df_door_nostop.apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]).lower())\n",
    "tdidf_ladybug_lem = df_ladybug_nostop.apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]).lower())\n",
    "tdidf_pain_lem = df_pain_nostop.apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]).lower())\n",
    "tdidf_blurriness_lem = df_blurriness_nostop.apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]).lower())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining the CountVectorizer vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining the CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "vectors_door = vectorizer.fit_transform(tdidf_door_lem)\n",
    "vectors_ladybug = vectorizer.fit_transform(tdidf_ladybug_lem)\n",
    "vectors_pain = vectorizer.fit_transform(tdidf_pain_lem)\n",
    "vectors_blurriness = vectorizer.fit_transform(tdidf_blurriness_lem)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the cosine similarity between the vectors of the same word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectors_similarity(vectors: list) -> float:\n",
    "    similarity_matrix = []\n",
    "    for row in vectors:\n",
    "        for row2 in vectors:\n",
    "            if np.array_equal(row, row2):\n",
    "                continue\n",
    "            similarity_matrix.append(distance.cosine(row, row2))\n",
    "    return round(1 - mean(similarity_matrix), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between all definitions with CountVectorizer\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Concrete</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Generic</th>\n",
       "      <td>pain<br> 0.14</td>\n",
       "      <td>door<br> 0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Specific</th>\n",
       "      <td>bluriness<br> 0.05</td>\n",
       "      <td>ladybug<br> 0.43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cosine similarity between all definitions of door, ladybug, pain and blurriness\n",
    "door_similarity = vectors_similarity(vectors_door.toarray())\n",
    "ladybug_similarity = vectors_similarity(vectors_ladybug.toarray())\n",
    "pain_similarity = vectors_similarity(vectors_pain.toarray())\n",
    "blurriness_similarity = vectors_similarity(vectors_blurriness.toarray())\n",
    "\n",
    "# I create the dataframe for printing the table with the results of the similarity with CountVectorizer\n",
    "data_cv = [['pain\\n {:.2f}'.format(pain_similarity), 'door\\n {:.2f}'.format(door_similarity)],\n",
    "              ['bluriness\\n {:.2f}'.format(blurriness_similarity), 'ladybug\\n {:.2f}'.format(ladybug_similarity)]]\n",
    "df_similarity_cv = pd.DataFrame(data_cv, columns=['Abstract', 'Concrete'], index=['Generic', 'Specific'])\n",
    "print(\"Similarity between all definitions with CountVectorizer\")\n",
    "display(HTML(df_similarity_cv.to_html().replace(\"\\\\n\",\"<br>\")))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
