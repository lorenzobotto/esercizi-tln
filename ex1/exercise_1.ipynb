{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from statistics import mean\n",
    "from IPython.display import display, HTML\n",
    "from nltk import WordNetLemmatizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/TLN-definitions-23.tsv', sep='\\t')\n",
    "df_door = df['door']\n",
    "df_ladybug = df['ladybug']\n",
    "df_pain = df['pain']\n",
    "df_blurriness = df['blurriness']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop words removal\n",
    "df_door_nostop = df_door.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "df_ladybug_nostop = df_ladybug.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "df_pain_nostop = df_pain.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "df_blurriness_nostop = df_blurriness.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "\n",
    "# Stemming and tokenization (with puntuaction removal)\n",
    "ps = PorterStemmer()\n",
    "df_door_stem = df_door_nostop.apply(\n",
    "    lambda x: tokenizer.tokenize(' '.join([ps.stem(word) for word in x.split()]).lower())\n",
    ")\n",
    "df_ladybug_stem = df_ladybug_nostop.apply(\n",
    "    lambda x: tokenizer.tokenize(' '.join([ps.stem(word) for word in x.split()]).lower())\n",
    ")\n",
    "df_pain_stem = df_pain_nostop.apply(\n",
    "    lambda x: tokenizer.tokenize(' '.join([ps.stem(word) for word in x.split()]).lower())\n",
    ")\n",
    "df_blurriness_stem = df_blurriness_nostop.apply(\n",
    "    lambda x: tokenizer.tokenize(' '.join([ps.stem(word) for word in x.split()]).lower())\n",
    ")\n",
    "\n",
    "# Lemmatization and tokenization (with puntuaction removal)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df_door_lem = df_door_nostop.apply(\n",
    "    lambda x: tokenizer.tokenize(' '.join([lemmatizer.lemmatize(word) for word in x.split()]).lower())\n",
    ")\n",
    "df_ladybug_lem = df_ladybug_nostop.apply(\n",
    "    lambda x: tokenizer.tokenize(' '.join([lemmatizer.lemmatize(word) for word in x.split()]).lower())\n",
    ")\n",
    "df_pain_lem = df_pain_nostop.apply(\n",
    "    lambda x: tokenizer.tokenize(' '.join([lemmatizer.lemmatize(word) for word in x.split()]).lower())\n",
    ")\n",
    "df_blurriness_lem = df_blurriness_nostop.apply(\n",
    "    lambda x: tokenizer.tokenize(' '.join([lemmatizer.lemmatize(word) for word in x.split()]).lower())\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for calculating the overlap between the definitions of two words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_overlap(set1: list, set2: list) -> int:\n",
    "    return len(set(set1).intersection(set(set2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def definitions_similarity(definitions: list) -> float:\n",
    "    similarity_matrix = []\n",
    "    for row in definitions:\n",
    "        for row2 in definitions:\n",
    "            if row == row2:\n",
    "                continue\n",
    "            overlap = calculate_overlap(row, row2)\n",
    "            overlap = overlap / (len(row) if len(row) < len(row2) else len(row2))\n",
    "            similarity_matrix.append(overlap)\n",
    "    return round(mean(similarity_matrix), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between all definitions with Stemming\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Concrete</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Generic</th>\n",
       "      <td>pain<br> 0.22</td>\n",
       "      <td>door<br> 0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Specific</th>\n",
       "      <td>bluriness<br> 0.08</td>\n",
       "      <td>ladybug<br> 0.56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between all definitions with Lemmatization\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Concrete</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Generic</th>\n",
       "      <td>pain<br> 0.20</td>\n",
       "      <td>door<br> 0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Specific</th>\n",
       "      <td>bluriness<br> 0.08</td>\n",
       "      <td>ladybug<br> 0.53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Overlap between all definitions of door, ladybug, pain and blurriness with both stemming and lemmatization\n",
    "door_similarity_stem = definitions_similarity(df_door_stem)\n",
    "ladybug_similarity_stem = definitions_similarity(df_ladybug_stem)\n",
    "pain_similarity_stem = definitions_similarity(df_pain_stem)\n",
    "blurriness_similarity_stem = definitions_similarity(df_blurriness_stem)\n",
    "\n",
    "door_similarity_lem = definitions_similarity(df_door_lem)\n",
    "ladybug_similarity_lem = definitions_similarity(df_ladybug_lem)\n",
    "pain_similarity_lem = definitions_similarity(df_pain_lem)\n",
    "blurriness_similarity_lem = definitions_similarity(df_blurriness_lem)\n",
    "\n",
    "# I create the dataframe for printing the table with the results of the similarity with stemming\n",
    "\n",
    "data_stem =   [['pain\\n {:.2f}'.format(pain_similarity_stem), 'door\\n {:.2f}'.format(door_similarity_stem)], \n",
    "             ['bluriness\\n {:.2f}'.format(blurriness_similarity_stem), 'ladybug\\n {:.2f}'.format(ladybug_similarity_stem)]]\n",
    "df_similarity_stem = pd.DataFrame(data_stem, columns=['Abstract', 'Concrete'], index=['Generic', 'Specific'])\n",
    "\n",
    "print(\"Similarity between all definitions with Stemming\")\n",
    "display(HTML(df_similarity_stem.to_html().replace(\"\\\\n\",\"<br>\")))\n",
    "\n",
    "# I create the dataframe for printing the table with the results of the similarity with lemmatization\n",
    "\n",
    "data_lem =  [['pain\\n {:.2f}'.format(pain_similarity_lem), 'door\\n {:.2f}'.format(door_similarity_lem)],\n",
    "                ['bluriness\\n {:.2f}'.format(blurriness_similarity_lem), 'ladybug\\n {:.2f}'.format(ladybug_similarity_lem)]]\n",
    "df_similarity_lem = pd.DataFrame(data_lem, columns=['Abstract', 'Concrete'], index=['Generic', 'Specific'])\n",
    "print(\"Similarity between all definitions with Lemmatization\")\n",
    "display(HTML(df_similarity_lem.to_html().replace(\"\\\\n\",\"<br>\")))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
