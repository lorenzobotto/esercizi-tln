{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from statistics import mean\n",
    "from IPython.display import display, HTML\n",
    "from nltk import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.spatial import distance\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/TLN-definitions-23.tsv', sep='\\t')\n",
    "df_door = df['door']\n",
    "df_ladybug = df['ladybug']\n",
    "df_pain = df['pain']\n",
    "df_blurriness = df['blurriness']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop words removal\n",
    "df_door_nostop = df_door.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "df_ladybug_nostop = df_ladybug.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "df_pain_nostop = df_pain.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "df_blurriness_nostop = df_blurriness.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "\n",
    "# Stemming and tokenization (with puntuaction removal)\n",
    "ps = PorterStemmer()\n",
    "df_door_stem = df_door_nostop.apply(\n",
    "    lambda x: tokenizer.tokenize(' '.join([ps.stem(word) for word in x.split()]).lower())\n",
    ")\n",
    "df_ladybug_stem = df_ladybug_nostop.apply(\n",
    "    lambda x: tokenizer.tokenize(' '.join([ps.stem(word) for word in x.split()]).lower())\n",
    ")\n",
    "df_pain_stem = df_pain_nostop.apply(\n",
    "    lambda x: tokenizer.tokenize(' '.join([ps.stem(word) for word in x.split()]).lower())\n",
    ")\n",
    "df_blurriness_stem = df_blurriness_nostop.apply(\n",
    "    lambda x: tokenizer.tokenize(' '.join([ps.stem(word) for word in x.split()]).lower())\n",
    ")\n",
    "\n",
    "# Lemmatization and tokenization (with puntuaction removal)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df_door_lem = df_door_nostop.apply(\n",
    "    lambda x: tokenizer.tokenize(' '.join([lemmatizer.lemmatize(word) for word in x.split()]).lower())\n",
    ")\n",
    "df_ladybug_lem = df_ladybug_nostop.apply(\n",
    "    lambda x: tokenizer.tokenize(' '.join([lemmatizer.lemmatize(word) for word in x.split()]).lower())\n",
    ")\n",
    "df_pain_lem = df_pain_nostop.apply(\n",
    "    lambda x: tokenizer.tokenize(' '.join([lemmatizer.lemmatize(word) for word in x.split()]).lower())\n",
    ")\n",
    "df_blurriness_lem = df_blurriness_nostop.apply(\n",
    "    lambda x: tokenizer.tokenize(' '.join([lemmatizer.lemmatize(word) for word in x.split()]).lower())\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Method: Overlap of words in the definitions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for calculating the overlap between the definitions of two words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_overlap(sentence_1: list, sentence_2: list) -> int:\n",
    "    return len(set(sentence_1).intersection(set(sentence_2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def definitions_similarity(definitions: list) -> float:\n",
    "    similarity_matrix = []\n",
    "    for sentence_1 in definitions:\n",
    "        for sentence_2 in definitions:\n",
    "            if sentence_1 == sentence_2:\n",
    "                continue\n",
    "            overlap = calculate_overlap(sentence_1, sentence_2)\n",
    "            overlap = overlap / (len(sentence_1) if len(sentence_1) < len(sentence_2) else len(sentence_2))\n",
    "            similarity_matrix.append(overlap)\n",
    "    return round(mean(similarity_matrix), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between all definitions with Stemming\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Concrete</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Generic</th>\n",
       "      <td>pain<br> 0.22</td>\n",
       "      <td>door<br> 0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Specific</th>\n",
       "      <td>bluriness<br> 0.08</td>\n",
       "      <td>ladybug<br> 0.56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between all definitions with Lemmatization\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Concrete</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Generic</th>\n",
       "      <td>pain<br> 0.20</td>\n",
       "      <td>door<br> 0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Specific</th>\n",
       "      <td>bluriness<br> 0.08</td>\n",
       "      <td>ladybug<br> 0.53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Overlap between all definitions of door, ladybug, pain and blurriness with both stemming and lemmatization\n",
    "door_similarity_stem = definitions_similarity(df_door_stem)\n",
    "ladybug_similarity_stem = definitions_similarity(df_ladybug_stem)\n",
    "pain_similarity_stem = definitions_similarity(df_pain_stem)\n",
    "blurriness_similarity_stem = definitions_similarity(df_blurriness_stem)\n",
    "\n",
    "door_similarity_lem = definitions_similarity(df_door_lem)\n",
    "ladybug_similarity_lem = definitions_similarity(df_ladybug_lem)\n",
    "pain_similarity_lem = definitions_similarity(df_pain_lem)\n",
    "blurriness_similarity_lem = definitions_similarity(df_blurriness_lem)\n",
    "\n",
    "# I create the dataframe for printing the table with the results of the similarity with stemming\n",
    "\n",
    "data_stem =  [['pain\\n {:.2f}'.format(pain_similarity_stem), 'door\\n {:.2f}'.format(door_similarity_stem)], \n",
    "              ['bluriness\\n {:.2f}'.format(blurriness_similarity_stem), 'ladybug\\n {:.2f}'.format(ladybug_similarity_stem)]]\n",
    "df_similarity_stem = pd.DataFrame(data_stem, columns=['Abstract', 'Concrete'], index=['Generic', 'Specific'])\n",
    "\n",
    "print(\"Similarity between all definitions with Stemming\")\n",
    "display(HTML(df_similarity_stem.to_html().replace(\"\\\\n\",\"<br>\")))\n",
    "\n",
    "# I create the dataframe for printing the table with the results of the similarity with lemmatization\n",
    "\n",
    "data_lem =  [['pain\\n {:.2f}'.format(pain_similarity_lem), 'door\\n {:.2f}'.format(door_similarity_lem)],\n",
    "             ['bluriness\\n {:.2f}'.format(blurriness_similarity_lem), 'ladybug\\n {:.2f}'.format(ladybug_similarity_lem)]]\n",
    "df_similarity_lem = pd.DataFrame(data_lem, columns=['Abstract', 'Concrete'], index=['Generic', 'Specific'])\n",
    "\n",
    "print(\"Similarity between all definitions with Lemmatization\")\n",
    "display(HTML(df_similarity_lem.to_html().replace(\"\\\\n\",\"<br>\")))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Method: Similarity of definitions with CountVectorizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "cv_door_lem = df_door_nostop.apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]).lower())\n",
    "cv_ladybug_lem = df_ladybug_nostop.apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]).lower())\n",
    "cv_pain_lem = df_pain_nostop.apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]).lower())\n",
    "cv_blurriness_lem = df_blurriness_nostop.apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]).lower())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining the CountVectorizer vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining the CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "vectors_door = vectorizer.fit_transform(cv_door_lem)\n",
    "vectors_ladybug = vectorizer.fit_transform(cv_ladybug_lem)\n",
    "vectors_pain = vectorizer.fit_transform(cv_pain_lem)\n",
    "vectors_blurriness = vectorizer.fit_transform(cv_blurriness_lem)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the cosine similarity between the vectors of the same word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectors_similarity(vectors: list) -> float:\n",
    "    similarity_matrix = []\n",
    "    for sentence_1 in vectors:\n",
    "        for sentence_2 in vectors:\n",
    "            if np.array_equal(sentence_1, sentence_2):\n",
    "                continue\n",
    "            similarity_matrix.append(distance.cosine(sentence_1, sentence_2))\n",
    "    return round(1 - mean(similarity_matrix), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between all definitions with CountVectorizer\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Concrete</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Generic</th>\n",
       "      <td>pain<br> 0.14</td>\n",
       "      <td>door<br> 0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Specific</th>\n",
       "      <td>bluriness<br> 0.05</td>\n",
       "      <td>ladybug<br> 0.43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cosine similarity between all definitions of door, ladybug, pain and blurriness\n",
    "door_similarity = vectors_similarity(vectors_door.toarray())\n",
    "ladybug_similarity = vectors_similarity(vectors_ladybug.toarray())\n",
    "pain_similarity = vectors_similarity(vectors_pain.toarray())\n",
    "blurriness_similarity = vectors_similarity(vectors_blurriness.toarray())\n",
    "\n",
    "# I create the dataframe for printing the table with the results of the similarity with CountVectorizer\n",
    "data_cv = [['pain\\n {:.2f}'.format(pain_similarity), 'door\\n {:.2f}'.format(door_similarity)],\n",
    "              ['bluriness\\n {:.2f}'.format(blurriness_similarity), 'ladybug\\n {:.2f}'.format(ladybug_similarity)]]\n",
    "df_similarity_cv = pd.DataFrame(data_cv, columns=['Abstract', 'Concrete'], index=['Generic', 'Specific'])\n",
    "print(\"Similarity between all definitions with CountVectorizer\")\n",
    "display(HTML(df_similarity_cv.to_html().replace(\"\\\\n\",\"<br>\")))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third Method (State of Art): Sentence Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity_tran(dataset: list) -> float:\n",
    "    # I load the transformer model\n",
    "    model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "    # I compute the embeddings for the dataset\n",
    "    embeddings = model.encode(dataset, convert_to_tensor=True)\n",
    "\n",
    "    # I compute the cosine similarity between all the embeddings and I return the mean\n",
    "    return round(util.torch.mean(util.cos_sim(embeddings, embeddings)).item(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between all definitions with SentenceTransformer\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Concrete</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Generic</th>\n",
       "      <td>pain<br> 0.60</td>\n",
       "      <td>door<br> 0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Specific</th>\n",
       "      <td>bluriness<br> 0.42</td>\n",
       "      <td>ladybug<br> 0.71</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cosine similarity between all definitions of door, ladybug, pain and blurriness\n",
    "door_similarity_tran = calculate_similarity_tran(df_door)\n",
    "ladybug_similarity_tran = calculate_similarity_tran(df_ladybug)\n",
    "pain_similarity_tran = calculate_similarity_tran(df_pain)\n",
    "blurriness_similarity_tran = calculate_similarity_tran(df_blurriness)\n",
    "\n",
    "# I create the dataframe for printing the table with the results of the similarity with Transformer\n",
    "data_tran = [['pain\\n {:.2f}'.format(pain_similarity_tran), 'door\\n {:.2f}'.format(door_similarity_tran)],\n",
    "              ['bluriness\\n {:.2f}'.format(blurriness_similarity_tran), 'ladybug\\n {:.2f}'.format(ladybug_similarity_tran)]]\n",
    "df_similarity_tran = pd.DataFrame(data_tran, columns=['Abstract', 'Concrete'], index=['Generic', 'Specific'])\n",
    "print(\"Similarity between all definitions with SentenceTransformer\")\n",
    "display(HTML(df_similarity_tran.to_html().replace(\"\\\\n\",\"<br>\")))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
