{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk import pos_tag\n",
    "import re\n",
    "\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/blogtext.csv', encoding='utf-8', nrows=5000)\n",
    "df.drop('id', axis=1, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "snow = SnowballStemmer('english')\n",
    "\n",
    "for sentence in df['text']:\n",
    "    sentence = sentence.lower()\n",
    "    sentence = sentence.replace(' mail ', ' ')\n",
    "    sentence = sentence.replace(' urllink ', ' ')\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    sentence = re.sub(cleanr, ' ', sentence)   # removing HTML tags\n",
    "    sentence = re.sub(r'[?|!|\\'|\"|#]', r'', sentence)\n",
    "    sentence = re.sub(r'[.|,|)|(|\\|/]', r'', sentence)  # removing punctuations\n",
    "    words = [snow.stem(word) for word in sentence.split() if word not in stop_words]\n",
    "    temp.append(words)\n",
    "\n",
    "final_text = temp.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = []\n",
    "for row in final_text:\n",
    "    seq = ''\n",
    "    for word in row:\n",
    "        seq = seq + ' ' + word\n",
    "    sent.append(seq)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I vectorize the documents using the TF-IDF vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(max_features=5000)\n",
    "X = vectorizer.fit_transform(sent)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA for topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA for topic modelling\n",
    "lda = LatentDirichletAllocation(n_components=20,\n",
    "                                learning_decay=0.5,         \n",
    "                                max_iter=50,                \n",
    "                                learning_method='online',   \n",
    "                                random_state=42,            \n",
    "                                batch_size=5000,            \n",
    "                                evaluate_every = -1,        \n",
    "                                n_jobs = -1) \n",
    "lda_output = lda.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:  ['la', 'de', 'un', 'que', 'el', 'en', 'su']\n",
      "Topic 1:  ['brought', 'court', 'quizilla', 'state', 'law', 'right', 'lott']\n",
      "Topic 2:  ['movi', 'http', 'film', 'new', 'mountain', 'star', 'one']\n",
      "Topic 3:  ['peopl', 'would', 'us', 'one', 'world', 'bush', 'war']\n",
      "Topic 4:  ['name', 'exam', 'histori', 'record', 'french', 'movi', 'us']\n",
      "Topic 5:  ['said', 'omar', 'offic', 'ask', 'prison', 'say', 'day']\n",
      "Topic 6:  ['john', 'bo', 'flower', 'man', 'name', 'red', 'like']\n",
      "Topic 7:  ['vote', 'would', 'suv', 'jonah', 'one', 'uncl', 'republican']\n",
      "Topic 8:  ['one', 'life', 'know', 'it', 'want', 'make', 'look']\n",
      "Topic 9:  ['korean', 'one', 'korea', 'like', 'get', 'bomb', 'bar']\n",
      "Topic 10:  ['job', 'pay', 'tax', 'pm', 'get', 'compani', 'flag']\n",
      "Topic 11:  ['place', 'one', 'would', 'look', 'like', 'see', 'could']\n",
      "Topic 12:  ['im', 'like', 'go', 'get', 'know', 'dont', 'think']\n",
      "Topic 13:  ['love', 'birthday', 'happi', 'day', 'favorit', 'music', 'god']\n",
      "Topic 14:  ['love', 'walk', 'eye', 'stop', 'way', 'one', 'bring']\n",
      "Topic 15:  ['post', 'blog', 'read', 'ben', 'site', 'link', 'book']\n",
      "Topic 16:  ['diva', 'good', 'danc', 'night', 'miami', 'weekend', 'im']\n",
      "Topic 17:  ['game', 'team', 'last', 'get', 'go', 'season', 'one']\n",
      "Topic 18:  ['go', 'dun', 'shrug', 'also', 'tt', 'bridg', 'wanna']\n",
      "Topic 19:  ['nbsp', 'like', 'song', 'go', 'would', 'got', 'one']\n"
     ]
    }
   ],
   "source": [
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "for index, component in enumerate(lda.components_):\n",
    "    zipped = zip(terms, component)\n",
    "    top_terms_key=sorted(zipped, key = lambda t: t[1], reverse=True)[:7]\n",
    "    top_terms_list=list(dict(top_terms_key).keys())\n",
    "    print(\"Topic \"+str(index)+\": \",top_terms_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
