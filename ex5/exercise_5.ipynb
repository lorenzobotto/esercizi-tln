{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import pos_tag\n",
    "import pyLDAvis\n",
    "\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/blogtext.csv', usecols=['topic', 'text'], encoding='utf-8', nrows=4000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(sentence: str) -> list:\n",
    "    sentence = sentence.lower()\n",
    "    sentence.replace(' mail ', ' ')\n",
    "    sentence.replace(' urllink ', ' ')\n",
    "    tokenizer = RegexpTokenizer(r\"[a-zA-Z0-9]+\")\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    filtered_words = [w for w in tokens if not w in stop_words and not w.isdigit()]\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_words = [stemmer.stem(w) for w in filtered_words]\n",
    "    nouns = [word for word, pos in pos_tag(stemmed_words) if pos.startswith('NN')]\n",
    "    return ' '.join(nouns)\n",
    "\n",
    "df['text'] = df['text'].apply(pre_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                          page mb leader process html\n",
       "1    team member der mail ruiyu xie mail bryan aald...\n",
       "2    het kader van kernfusi je eigen build h bomb r...\n",
       "3                                            test test\n",
       "4    thank yahoo toolbar captur popup mean show pop...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I vectorize the documents using the TF-IDF vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(df['text'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA for topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA for topic modelling\n",
    "lda = LatentDirichletAllocation(n_components=20,\n",
    "                                learning_decay=0.5,         \n",
    "                                max_iter=50,                \n",
    "                                learning_method='online',   \n",
    "                                random_state=42,            \n",
    "                                batch_size=5000,            \n",
    "                                evaluate_every = -1,        \n",
    "                                n_jobs = -1) \n",
    "lda_output = lda.fit_transform(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the topics with pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Likelihood:  -899224.3097723977\n",
      "Perplexity:  3320.8834647388862\n",
      "{'batch_size': 5000, 'doc_topic_prior': None, 'evaluate_every': -1, 'learning_decay': 0.5, 'learning_method': 'online', 'learning_offset': 10.0, 'max_doc_update_iter': 100, 'max_iter': 50, 'mean_change_tol': 0.001, 'n_components': 20, 'n_jobs': -1, 'perp_tol': 0.1, 'random_state': 42, 'topic_word_prior': None, 'total_samples': 1000000.0, 'verbose': 0}\n"
     ]
    }
   ],
   "source": [
    "# Log Likelyhood: Higher the better\n",
    "print(\"Log Likelihood: \", lda.score(X))\n",
    "# Perplexity: Lower the better. Perplexity = exp(-1. * log-likelihood per word)\n",
    "print(\"Perplexity: \", lda.perplexity(X))\n",
    "# See model parameters\n",
    "print(lda.get_params())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
