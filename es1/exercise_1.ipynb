{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Part\n",
    "- The exercise consists of implementing three similarity measures based on WordNet:\n",
    "    - WU & Palmer similarity\n",
    "    - Shortest Path similarity\n",
    "    - Leacock & Chodorow similarity\n",
    "- For each of these similarity measures, calculate Spearman's correlation indices and Pearson's correlation indices between the results obtained and the 'target' results in the dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necessary imports and dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lores\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "import nltk\n",
    "from nltk.corpus.reader.wordnet import Synset\n",
    "nltk.download('wordnet')\n",
    "import pandas as pd\n",
    "import math\n",
    "from scipy import stats as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word 1</th>\n",
       "      <th>Word 2</th>\n",
       "      <th>Human (mean)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>love</td>\n",
       "      <td>sex</td>\n",
       "      <td>6.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tiger</td>\n",
       "      <td>cat</td>\n",
       "      <td>7.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tiger</td>\n",
       "      <td>tiger</td>\n",
       "      <td>10.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>book</td>\n",
       "      <td>paper</td>\n",
       "      <td>7.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>computer</td>\n",
       "      <td>keyboard</td>\n",
       "      <td>7.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>shower</td>\n",
       "      <td>flood</td>\n",
       "      <td>6.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>weather</td>\n",
       "      <td>forecast</td>\n",
       "      <td>8.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>disaster</td>\n",
       "      <td>area</td>\n",
       "      <td>6.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>governor</td>\n",
       "      <td>office</td>\n",
       "      <td>6.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>architecture</td>\n",
       "      <td>century</td>\n",
       "      <td>3.78</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>353 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Word 1    Word 2  Human (mean)\n",
       "0            love       sex          6.77\n",
       "1           tiger       cat          7.35\n",
       "2           tiger     tiger         10.00\n",
       "3            book     paper          7.46\n",
       "4        computer  keyboard          7.62\n",
       "..            ...       ...           ...\n",
       "348        shower     flood          6.03\n",
       "349       weather  forecast          8.34\n",
       "350      disaster      area          6.25\n",
       "351      governor    office          6.34\n",
       "352  architecture   century          3.78\n",
       "\n",
       "[353 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reading the file\n",
    "df = pd.read_csv('dataset\\combined.csv')\n",
    "display(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the function to get the max depth of the synset\n",
    "\n",
    "def max_depth_synset(synset: Synset) -> int:\n",
    "    return max(len(path) for path in synset.hypernym_paths())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the function to get the least common subsumer of two synsets\n",
    "\n",
    "def LCS(syn1: Synset, syn2: Synset) -> Synset | None:\n",
    "    path1 = syn1.hypernym_paths()\n",
    "    path2 = syn2.hypernym_paths()\n",
    "    result = []\n",
    "\n",
    "    # We loop through the paths of the first synset\n",
    "    for pat1 in path1:\n",
    "        x = pat1\n",
    "        x.reverse()\n",
    "        found = False\n",
    "        for syn in x:\n",
    "            if(not found):\n",
    "                # We loop through the paths of the second synset\n",
    "                for pat2 in path2:\n",
    "                    if(syn in pat2):\n",
    "                        result.append(syn)\n",
    "                        found = True\n",
    "                        break\n",
    "            else:\n",
    "                break\n",
    "    if not result:\n",
    "            return None\n",
    "    else:\n",
    "         max_depth = max(max_depth_synset(s) for s in result)\n",
    "         max_hypernym = max(s for s in result if max_depth_synset(s) == max_depth)\n",
    "         return max_hypernym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_len(syn1: Synset, syn2: Synset) -> int:\n",
    "    path1 = syn1.hypernym_paths()\n",
    "    path2 = syn2.hypernym_paths()\n",
    "    result_syn1 = []\n",
    "    result_syn2 = []\n",
    "\n",
    "    common_ancestor = LCS(syn1,syn2)\n",
    "\n",
    "    if(common_ancestor is None):\n",
    "        return None\n",
    "    else:\n",
    "        for pat1 in path1:\n",
    "            if(common_ancestor in pat1):\n",
    "                result_syn1.append(pat1.index(syn1)-pat1.index(common_ancestor))\n",
    "        for pat2 in path2:\n",
    "            if(common_ancestor in pat2):\n",
    "                result_syn2.append(pat2.index(syn2)-pat2.index(common_ancestor))\n",
    "        return min(result_syn1) + min(result_syn2)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_max = max(max(len(hyp_path) for hyp_path in ss.hypernym_paths()) for ss in wn.all_synsets())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wu Palmer\n",
    "\n",
    "$$ LCS(s1, s2) = {2 * depth(LCS(s1,s2)) \\over depth(s1) + depth(s2)} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wu_palmer(syn1: Synset, syn2: Synset) -> float:\n",
    "    # We get all the synsets of the two words\n",
    "    depht1 = max_depth_synset(syn1)\n",
    "    depht2 = max_depth_synset(syn2)\n",
    "    # Finding the least common subsumer of the two synsets\n",
    "    common_ancestor = LCS(syn1,syn2)\n",
    "    if common_ancestor is None:\n",
    "        return 0\n",
    "    return 2 * max_depth_synset(common_ancestor) / ( depht1 + depht2 )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shortest Path\n",
    "\n",
    "$$ LCS(s1, s2) = 2 * depthMax - len(s1,s2) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shortest_path(syn1: Synset, syn2: Synset) -> int:\n",
    "    pat = path_len(syn1,syn2)\n",
    "    if pat is None:\n",
    "        return 0\n",
    "    elif (pat == 0):\n",
    "        return 2 * depth_max\n",
    "    elif (pat == depth_max):\n",
    "        return 0\n",
    "    else:\n",
    "        return 2 * depth_max - pat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leakcock & Chodorow\n",
    "\n",
    "$$ sim_LC(s1, s2) = -log(\\frac{len(s1,s2)}{2*depthMax}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leakcock_chodorow (syn1: Synset, syn2: Synset) -> float:\n",
    "    pat = path_len(syn1,syn2)\n",
    "    if pat is None:\n",
    "        return 0\n",
    "    elif (pat == 0):\n",
    "        return -math.log((1)/(2 * depth_max + 1))\n",
    "    else:\n",
    "        return -math.log((pat)/(2 * depth_max))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cycle for every synset of the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synsets_cycle(word1: str, word2: str, metric: callable) -> float | str:\n",
    "    syn1 = wn.synsets(word1)\n",
    "    syn2 = wn.synsets(word2)\n",
    "    result = []\n",
    "    for s1 in syn1:\n",
    "        for s2 in syn2:\n",
    "            result.append(metric(s1,s2))\n",
    "    if not result:\n",
    "        return 0\n",
    "    else:\n",
    "        return max(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WU Palmer between dog and cat: 0.8571428571428571\n",
      "WU Palmer between stock and live: 0.2857142857142857\n",
      "Shortest Path between Jerusalem and Palestinian: 24\n",
      "Leakcock Chodorow between dog and cat: 2.3025850929940455\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "print(\"WU Palmer between dog and cat:\", synsets_cycle('dog','cat', wu_palmer))\n",
    "print(\"WU Palmer between stock and live:\", synsets_cycle('stock','live', wu_palmer))\n",
    "print(\"Shortest Path between Jerusalem and Palestinian:\", synsets_cycle('Jerusalem','Palestinian', shortest_path))\n",
    "print(\"Leakcock Chodorow between dog and cat:\", synsets_cycle('dog','cat', leakcock_chodorow))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop over all pair of words in the dataset and i print the three similarity measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word 1</th>\n",
       "      <th>Word 2</th>\n",
       "      <th>Human (mean)</th>\n",
       "      <th>WU Palmer</th>\n",
       "      <th>Shortest Path</th>\n",
       "      <th>Leakcock Chodorow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>love</td>\n",
       "      <td>sex</td>\n",
       "      <td>6.77</td>\n",
       "      <td>9.230769</td>\n",
       "      <td>9.75</td>\n",
       "      <td>9.933507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tiger</td>\n",
       "      <td>cat</td>\n",
       "      <td>7.35</td>\n",
       "      <td>9.655172</td>\n",
       "      <td>9.75</td>\n",
       "      <td>9.933507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tiger</td>\n",
       "      <td>tiger</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>book</td>\n",
       "      <td>paper</td>\n",
       "      <td>7.46</td>\n",
       "      <td>8.750000</td>\n",
       "      <td>9.50</td>\n",
       "      <td>8.066983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>computer</td>\n",
       "      <td>keyboard</td>\n",
       "      <td>7.62</td>\n",
       "      <td>8.235294</td>\n",
       "      <td>9.25</td>\n",
       "      <td>6.975136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>shower</td>\n",
       "      <td>flood</td>\n",
       "      <td>6.03</td>\n",
       "      <td>6.363636</td>\n",
       "      <td>9.00</td>\n",
       "      <td>6.200459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>weather</td>\n",
       "      <td>forecast</td>\n",
       "      <td>8.34</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>6.75</td>\n",
       "      <td>3.026547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>disaster</td>\n",
       "      <td>area</td>\n",
       "      <td>6.25</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>8.00</td>\n",
       "      <td>4.333935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>governor</td>\n",
       "      <td>office</td>\n",
       "      <td>6.34</td>\n",
       "      <td>5.263158</td>\n",
       "      <td>7.75</td>\n",
       "      <td>4.016766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>architecture</td>\n",
       "      <td>century</td>\n",
       "      <td>3.78</td>\n",
       "      <td>3.076923</td>\n",
       "      <td>7.75</td>\n",
       "      <td>4.016766</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>353 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Word 1    Word 2  Human (mean)  WU Palmer  Shortest Path   \n",
       "0            love       sex          6.77   9.230769           9.75  \\\n",
       "1           tiger       cat          7.35   9.655172           9.75   \n",
       "2           tiger     tiger         10.00  10.000000          10.00   \n",
       "3            book     paper          7.46   8.750000           9.50   \n",
       "4        computer  keyboard          7.62   8.235294           9.25   \n",
       "..            ...       ...           ...        ...            ...   \n",
       "348        shower     flood          6.03   6.363636           9.00   \n",
       "349       weather  forecast          8.34   1.333333           6.75   \n",
       "350      disaster      area          6.25   5.000000           8.00   \n",
       "351      governor    office          6.34   5.263158           7.75   \n",
       "352  architecture   century          3.78   3.076923           7.75   \n",
       "\n",
       "     Leakcock Chodorow  \n",
       "0             9.933507  \n",
       "1             9.933507  \n",
       "2            10.000000  \n",
       "3             8.066983  \n",
       "4             6.975136  \n",
       "..                 ...  \n",
       "348           6.200459  \n",
       "349           3.026547  \n",
       "350           4.333935  \n",
       "351           4.016766  \n",
       "352           4.016766  \n",
       "\n",
       "[353 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loop through the dataset and calculate the similarity for each pair of words using the three metrics and i add the result to a new dataframe\n",
    "df_metrics = pd.DataFrame(columns=['Word 1', 'Word 2', \"Human (mean)\", \"WU Palmer\", \"Shortest Path\", \"Leakcock Chodorow\"])\n",
    "for index, row in df.iterrows():\n",
    "    df_metrics.loc[len(df_metrics)] = {'Word 1': row['Word 1'], 'Word 2': row['Word 2'], \"Human (mean)\": row['Human (mean)'], \"WU Palmer\": synsets_cycle(row['Word 1'], row['Word 2'], wu_palmer) * 10, \"Shortest Path\": (synsets_cycle(row['Word 1'], row['Word 2'], shortest_path) /40) * 10, \"Leakcock Chodorow\": (synsets_cycle(row['Word 1'], row['Word 2'], leakcock_chodorow) / math.log(41)) * 10}\n",
    "display(df_metrics)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation of correlation with Spearman and Pearson metrics between the human similarity and the similarity calculated by the three metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "WU Palmer Metric:\n",
      "Spearman Correlation: SignificanceResult(statistic=0.34531446776235897, pvalue=2.5376440594705738e-11)\n",
      "The distribution of the WU Palmer metric is significantly different from the distribution of the Human Similarity.\n",
      "\n",
      "Pearson Correlation: PearsonRResult(statistic=0.28732609798215053, pvalue=3.895408936985221e-08)\n",
      "The distribution of the WU Palmer metric is significantly different from the distribution of the Human Similarity.\n",
      "-------------------------------------------\n",
      "Shortest Path Metric:\n",
      "Spearman Correlation: SignificanceResult(statistic=0.2883010732370922, pvalue=3.4876313366885126e-08)\n",
      "The distribution of the Shortest Path metric is significantly different from the distribution of the Human Similarity.\n",
      "\n",
      "Pearson Correlation: PearsonRResult(statistic=0.16411938908972765, pvalue=0.0019775111177550756)\n",
      "The distribution of the Shortest Path metric is significantly different from the distribution of the Human Similarity.\n",
      "-------------------------------------------\n",
      "Leakcock Chodorow Metric:\n",
      "Spearman Correlation: SignificanceResult(statistic=0.2883010732370922, pvalue=3.4876313366885126e-08)\n",
      "The distribution of the Leakcock Chodorow metric is significantly different from the distribution of the Human Similarity.\n",
      "\n",
      "Pearson Correlation: PearsonRResult(statistic=0.317375647242182, pvalue=1.0592028515546679e-09)\n",
      "The distribution of the Leakcock Chodorow metric is significantly different from the distribution of the Human Similarity.\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Calculate the correlation between the human similarity and the similarity calculated by the three metrics\n",
    "\n",
    "print(\"-------------------------------------------\")\n",
    "print(\"WU Palmer Metric:\")\n",
    "spearman_correlation = st.spearmanr(df_metrics[\"WU Palmer\"], df_metrics[\"Human (mean)\"])\n",
    "print(\"Spearman Correlation:\", spearman_correlation)\n",
    "if spearman_correlation[1] < 0.05:\n",
    "    print(\"The distribution of the WU Palmer metric is significantly different from the distribution of the Human Similarity.\\n\")\n",
    "else:\n",
    "    print(\"The distribution of the WU Palmer metric is almost the same as the distribution of the Human Similarity.\\n\")\n",
    "pearson_correlation = st.pearsonr(df_metrics[\"WU Palmer\"], df_metrics[\"Human (mean)\"])\n",
    "print(\"Pearson Correlation:\", pearson_correlation)\n",
    "if pearson_correlation[1] < 0.05:\n",
    "    print(\"The distribution of the WU Palmer metric is significantly different from the distribution of the Human Similarity.\")\n",
    "else:\n",
    "    print(\"The distribution of the WU Palmer metric is almost the same as the distribution of the Human Similarity.\")\n",
    "print(\"-------------------------------------------\")\n",
    "\n",
    "print(\"Shortest Path Metric:\")\n",
    "spearman_correlation = st.spearmanr(df_metrics[\"Shortest Path\"], df_metrics[\"Human (mean)\"])\n",
    "print(\"Spearman Correlation:\", spearman_correlation)\n",
    "if spearman_correlation[1] < 0.05:\n",
    "    print(\"The distribution of the Shortest Path metric is significantly different from the distribution of the Human Similarity.\\n\")\n",
    "else:\n",
    "    print(\"The distribution of the Shortest Path metric is almost the same as the distribution of the Human Similarity.\\n\")\n",
    "pearson_correlation = st.pearsonr(df_metrics[\"Shortest Path\"], df_metrics[\"Human (mean)\"])\n",
    "print(\"Pearson Correlation:\", pearson_correlation)\n",
    "if pearson_correlation[1] < 0.05:\n",
    "    print(\"The distribution of the Shortest Path metric is significantly different from the distribution of the Human Similarity.\")\n",
    "else:\n",
    "    print(\"The distribution of the Shortest Path metric is almost the same as the distribution of the Human Similarity.\")\n",
    "print(\"-------------------------------------------\")\n",
    "\n",
    "print(\"Leakcock Chodorow Metric:\")\n",
    "spearman_correlation = st.spearmanr(df_metrics[\"Leakcock Chodorow\"], df_metrics[\"Human (mean)\"])\n",
    "print(\"Spearman Correlation:\", spearman_correlation)\n",
    "if spearman_correlation[1] < 0.05:\n",
    "    print(\"The distribution of the Leakcock Chodorow metric is significantly different from the distribution of the Human Similarity.\\n\")\n",
    "else:\n",
    "    print(\"The distribution of the Leakcock Chodorow metric is almost the same as the distribution of the Human Similarity.\\n\")\n",
    "pearson_correlation = st.pearsonr(df_metrics[\"Leakcock Chodorow\"], df_metrics[\"Human (mean)\"])\n",
    "print(\"Pearson Correlation:\", pearson_correlation)\n",
    "if pearson_correlation[1] < 0.05:\n",
    "    print(\"The distribution of the Leakcock Chodorow metric is significantly different from the distribution of the Human Similarity.\")\n",
    "else:\n",
    "    print(\"The distribution of the Leakcock Chodorow metric is almost the same as the distribution of the Human Similarity.\")\n",
    "print(\"-------------------------------------------\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Part\n",
    "\n",
    "- Implementing Lesk's algorithm (from NLTK).\n",
    "    1. Extract 50 sentences from the SemCor corpus (corpus annotated with WN synsets) and disambiguate (at least) one noun per sentence. Calculate the accuracy of the implemented system based on the senses annotated in SemCor.\n",
    "    SemCor is available at the URL: http://web.eecs.umich.edu/~mihalcea/downloads.html\n",
    "    2. Randomize the selection of the 50 sentences and the selection of the term to be disambiguated, and return the average accuracy over (for example) 10 runs of the program.\n",
    "- [OPTIONAL]: implement corpus-Lesk algorithm using Sem-Cor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
