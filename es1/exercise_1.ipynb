{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\gabri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package semcor to\n",
      "[nltk_data]     C:\\Users\\gabri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package semcor is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gabri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\gabri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gabri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import semcor\n",
    "from nltk.corpus.reader.wordnet import Synset\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import math\n",
    "import re\n",
    "from scipy import stats as st\n",
    "import random\n",
    "from nltk.corpus.reader.api import *\n",
    "from nltk.corpus.reader.xmldocs import XMLCorpusReader, XMLCorpusView\n",
    "from nltk.tree import Tree\n",
    "nltk.download('wordnet')\n",
    "nltk.download('semcor')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Part\n",
    "- The exercise consists of implementing three similarity measures based on WordNet:\n",
    "    - WU & Palmer similarity\n",
    "    - Shortest Path similarity\n",
    "    - Leacock & Chodorow similarity\n",
    "- For each of these similarity measures, calculate Spearman's correlation indices and Pearson's correlation indices between the results obtained and the 'target' results in the dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word 1</th>\n",
       "      <th>Word 2</th>\n",
       "      <th>Human (mean)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>love</td>\n",
       "      <td>sex</td>\n",
       "      <td>6.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tiger</td>\n",
       "      <td>cat</td>\n",
       "      <td>7.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tiger</td>\n",
       "      <td>tiger</td>\n",
       "      <td>10.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>book</td>\n",
       "      <td>paper</td>\n",
       "      <td>7.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>computer</td>\n",
       "      <td>keyboard</td>\n",
       "      <td>7.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>shower</td>\n",
       "      <td>flood</td>\n",
       "      <td>6.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>weather</td>\n",
       "      <td>forecast</td>\n",
       "      <td>8.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>disaster</td>\n",
       "      <td>area</td>\n",
       "      <td>6.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>governor</td>\n",
       "      <td>office</td>\n",
       "      <td>6.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>architecture</td>\n",
       "      <td>century</td>\n",
       "      <td>3.78</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>353 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Word 1    Word 2  Human (mean)\n",
       "0            love       sex          6.77\n",
       "1           tiger       cat          7.35\n",
       "2           tiger     tiger         10.00\n",
       "3            book     paper          7.46\n",
       "4        computer  keyboard          7.62\n",
       "..            ...       ...           ...\n",
       "348        shower     flood          6.03\n",
       "349       weather  forecast          8.34\n",
       "350      disaster      area          6.25\n",
       "351      governor    office          6.34\n",
       "352  architecture   century          3.78\n",
       "\n",
       "[353 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reading the file\n",
    "df = pd.read_csv('dataset\\combined.csv')\n",
    "display(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the function to get the max depth of the synset\n",
    "\n",
    "def max_depth_synset(synset: Synset) -> int:\n",
    "    return max(len(path) for path in synset.hypernym_paths())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the function to get the least common subsumer of two synsets\n",
    "\n",
    "def LCS(syn1: Synset, syn2: Synset) -> Synset | None:\n",
    "    path1 = syn1.hypernym_paths()\n",
    "    path2 = syn2.hypernym_paths()\n",
    "    result = []\n",
    "\n",
    "    # We loop through the paths of the first synset\n",
    "    for pat1 in path1:\n",
    "        x = pat1\n",
    "        x.reverse()\n",
    "        found = False\n",
    "        for syn in x:\n",
    "            if(not found):\n",
    "                # We loop through the paths of the second synset\n",
    "                for pat2 in path2:\n",
    "                    if(syn in pat2):\n",
    "                        result.append(syn)\n",
    "                        found = True\n",
    "                        break\n",
    "            else:\n",
    "                break\n",
    "    if not result:\n",
    "            return None\n",
    "    else:\n",
    "         max_depth = max(max_depth_synset(s) for s in result)\n",
    "         max_hypernym = max(s for s in result if max_depth_synset(s) == max_depth)\n",
    "         return max_hypernym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_len(syn1: Synset, syn2: Synset) -> int:\n",
    "    path1 = syn1.hypernym_paths()\n",
    "    path2 = syn2.hypernym_paths()\n",
    "    result_syn1 = []\n",
    "    result_syn2 = []\n",
    "\n",
    "    common_ancestor = LCS(syn1,syn2)\n",
    "\n",
    "    if(common_ancestor is None):\n",
    "        return None\n",
    "    else:\n",
    "        for pat1 in path1:\n",
    "            if(common_ancestor in pat1):\n",
    "                result_syn1.append(pat1.index(syn1)-pat1.index(common_ancestor))\n",
    "        for pat2 in path2:\n",
    "            if(common_ancestor in pat2):\n",
    "                result_syn2.append(pat2.index(syn2)-pat2.index(common_ancestor))\n",
    "        return min(result_syn1) + min(result_syn2)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\gabri\\Desktop\\esercizi-radicioni\\es1\\exercise_1.ipynb Cell 11\u001b[0m in \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/gabri/Desktop/esercizi-radicioni/es1/exercise_1.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m depth_max \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(\u001b[39mmax\u001b[39m(\u001b[39mlen\u001b[39m(hyp_path) \u001b[39mfor\u001b[39;00m hyp_path \u001b[39min\u001b[39;00m ss\u001b[39m.\u001b[39mhypernym_paths()) \u001b[39mfor\u001b[39;00m ss \u001b[39min\u001b[39;00m wn\u001b[39m.\u001b[39mall_synsets())\n",
      "\u001b[1;32mc:\\Users\\gabri\\Desktop\\esercizi-radicioni\\es1\\exercise_1.ipynb Cell 11\u001b[0m in \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/gabri/Desktop/esercizi-radicioni/es1/exercise_1.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m depth_max \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(\u001b[39mmax\u001b[39m(\u001b[39mlen\u001b[39m(hyp_path) \u001b[39mfor\u001b[39;00m hyp_path \u001b[39min\u001b[39;00m ss\u001b[39m.\u001b[39mhypernym_paths()) \u001b[39mfor\u001b[39;00m ss \u001b[39min\u001b[39;00m wn\u001b[39m.\u001b[39mall_synsets())\n",
      "File \u001b[1;32mc:\\Users\\gabri\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py:1883\u001b[0m, in \u001b[0;36mWordNetCorpusReader.all_eng_synsets\u001b[1;34m(self, pos)\u001b[0m\n\u001b[0;32m   1880\u001b[0m     synset \u001b[39m=\u001b[39m cache[pos_tag][offset]\n\u001b[0;32m   1881\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1882\u001b[0m     \u001b[39m# Otherwise, parse the line\u001b[39;00m\n\u001b[1;32m-> 1883\u001b[0m     synset \u001b[39m=\u001b[39m from_pos_and_line(pos_tag, line)\n\u001b[0;32m   1884\u001b[0m     cache[pos_tag][offset] \u001b[39m=\u001b[39m synset\n\u001b[0;32m   1886\u001b[0m \u001b[39m# adjective satellites are in the same file as\u001b[39;00m\n\u001b[0;32m   1887\u001b[0m \u001b[39m# adjectives so only yield the synset if it's actually\u001b[39;00m\n\u001b[0;32m   1888\u001b[0m \u001b[39m# a satellite\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\gabri\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py:1600\u001b[0m, in \u001b[0;36mWordNetCorpusReader._synset_from_pos_and_line\u001b[1;34m(self, pos, data_file_line)\u001b[0m\n\u001b[0;32m   1597\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39m(_iter)\n\u001b[0;32m   1599\u001b[0m \u001b[39m# get the offset\u001b[39;00m\n\u001b[1;32m-> 1600\u001b[0m synset\u001b[39m.\u001b[39m_offset \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(_next_token())\n\u001b[0;32m   1602\u001b[0m \u001b[39m# determine the lexicographer file name\u001b[39;00m\n\u001b[0;32m   1603\u001b[0m lexname_index \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(_next_token())\n",
      "File \u001b[1;32mc:\\Users\\gabri\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py:1597\u001b[0m, in \u001b[0;36mWordNetCorpusReader._synset_from_pos_and_line.<locals>._next_token\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1596\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_token\u001b[39m():\n\u001b[1;32m-> 1597\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39;49m(_iter)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "depth_max = max(max(len(hyp_path) for hyp_path in ss.hypernym_paths()) for ss in wn.all_synsets())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wu Palmer\n",
    "\n",
    "$$ LCS(s1, s2) = {2 * depth(LCS(s1,s2)) \\over depth(s1) + depth(s2)} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wu_palmer(syn1: Synset, syn2: Synset) -> float:\n",
    "    # We get all the synsets of the two words\n",
    "    depht1 = max_depth_synset(syn1)\n",
    "    depht2 = max_depth_synset(syn2)\n",
    "    # Finding the least common subsumer of the two synsets\n",
    "    common_ancestor = LCS(syn1,syn2)\n",
    "    if common_ancestor is None:\n",
    "        return 0\n",
    "    return 2 * max_depth_synset(common_ancestor) / ( depht1 + depht2 )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shortest Path\n",
    "\n",
    "$$ LCS(s1, s2) = 2 * depthMax - len(s1,s2) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shortest_path(syn1: Synset, syn2: Synset) -> int:\n",
    "    pat = path_len(syn1,syn2)\n",
    "    if pat is None:\n",
    "        return 0\n",
    "    elif (pat == 0):\n",
    "        return 2 * depth_max\n",
    "    elif (pat == depth_max):\n",
    "        return 0\n",
    "    else:\n",
    "        return 2 * depth_max - pat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leakcock & Chodorow\n",
    "\n",
    "$$ sim_LC(s1, s2) = -log(\\frac{len(s1,s2)}{2*depthMax}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leakcock_chodorow (syn1: Synset, syn2: Synset) -> float:\n",
    "    pat = path_len(syn1,syn2)\n",
    "    if pat is None:\n",
    "        return 0\n",
    "    elif (pat == 0):\n",
    "        return -math.log((1)/(2 * depth_max + 1))\n",
    "    else:\n",
    "        return -math.log((pat)/(2 * depth_max))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cycle for every synset of the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synsets_cycle(word1: str, word2: str, metric: callable) -> float | str:\n",
    "    syn1 = wn.synsets(word1)\n",
    "    syn2 = wn.synsets(word2)\n",
    "    result = []\n",
    "    for s1 in syn1:\n",
    "        for s2 in syn2:\n",
    "            result.append(metric(s1,s2))\n",
    "    if not result:\n",
    "        return 0\n",
    "    else:\n",
    "        return max(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WU Palmer between dog and cat: 0.8571428571428571\n",
      "WU Palmer between stock and live: 0.2857142857142857\n",
      "Shortest Path between Jerusalem and Palestinian: 24\n",
      "Leakcock Chodorow between dog and cat: 2.3025850929940455\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "print(\"WU Palmer between dog and cat:\", synsets_cycle('dog','cat', wu_palmer))\n",
    "print(\"WU Palmer between stock and live:\", synsets_cycle('stock','live', wu_palmer))\n",
    "print(\"Shortest Path between Jerusalem and Palestinian:\", synsets_cycle('Jerusalem','Palestinian', shortest_path))\n",
    "print(\"Leakcock Chodorow between dog and cat:\", synsets_cycle('dog','cat', leakcock_chodorow))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop over all pair of words in the dataset and i print the three similarity measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word 1</th>\n",
       "      <th>Word 2</th>\n",
       "      <th>Human (mean)</th>\n",
       "      <th>WU Palmer</th>\n",
       "      <th>Shortest Path</th>\n",
       "      <th>Leakcock Chodorow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>love</td>\n",
       "      <td>sex</td>\n",
       "      <td>6.77</td>\n",
       "      <td>9.230769</td>\n",
       "      <td>9.75</td>\n",
       "      <td>9.933507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tiger</td>\n",
       "      <td>cat</td>\n",
       "      <td>7.35</td>\n",
       "      <td>9.655172</td>\n",
       "      <td>9.75</td>\n",
       "      <td>9.933507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tiger</td>\n",
       "      <td>tiger</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>book</td>\n",
       "      <td>paper</td>\n",
       "      <td>7.46</td>\n",
       "      <td>8.750000</td>\n",
       "      <td>9.50</td>\n",
       "      <td>8.066983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>computer</td>\n",
       "      <td>keyboard</td>\n",
       "      <td>7.62</td>\n",
       "      <td>8.235294</td>\n",
       "      <td>9.25</td>\n",
       "      <td>6.975136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>shower</td>\n",
       "      <td>flood</td>\n",
       "      <td>6.03</td>\n",
       "      <td>6.363636</td>\n",
       "      <td>9.00</td>\n",
       "      <td>6.200459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>weather</td>\n",
       "      <td>forecast</td>\n",
       "      <td>8.34</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>6.75</td>\n",
       "      <td>3.026547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>disaster</td>\n",
       "      <td>area</td>\n",
       "      <td>6.25</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>8.00</td>\n",
       "      <td>4.333935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>governor</td>\n",
       "      <td>office</td>\n",
       "      <td>6.34</td>\n",
       "      <td>5.263158</td>\n",
       "      <td>7.75</td>\n",
       "      <td>4.016766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>architecture</td>\n",
       "      <td>century</td>\n",
       "      <td>3.78</td>\n",
       "      <td>3.076923</td>\n",
       "      <td>7.75</td>\n",
       "      <td>4.016766</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>353 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Word 1    Word 2  Human (mean)  WU Palmer  Shortest Path   \n",
       "0            love       sex          6.77   9.230769           9.75  \\\n",
       "1           tiger       cat          7.35   9.655172           9.75   \n",
       "2           tiger     tiger         10.00  10.000000          10.00   \n",
       "3            book     paper          7.46   8.750000           9.50   \n",
       "4        computer  keyboard          7.62   8.235294           9.25   \n",
       "..            ...       ...           ...        ...            ...   \n",
       "348        shower     flood          6.03   6.363636           9.00   \n",
       "349       weather  forecast          8.34   1.333333           6.75   \n",
       "350      disaster      area          6.25   5.000000           8.00   \n",
       "351      governor    office          6.34   5.263158           7.75   \n",
       "352  architecture   century          3.78   3.076923           7.75   \n",
       "\n",
       "     Leakcock Chodorow  \n",
       "0             9.933507  \n",
       "1             9.933507  \n",
       "2            10.000000  \n",
       "3             8.066983  \n",
       "4             6.975136  \n",
       "..                 ...  \n",
       "348           6.200459  \n",
       "349           3.026547  \n",
       "350           4.333935  \n",
       "351           4.016766  \n",
       "352           4.016766  \n",
       "\n",
       "[353 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loop through the dataset and calculate the similarity for each pair of words using the three metrics and i add the result to a new dataframe\n",
    "df_metrics = pd.DataFrame(columns=['Word 1', 'Word 2', \"Human (mean)\", \"WU Palmer\", \"Shortest Path\", \"Leakcock Chodorow\"])\n",
    "for index, row in df.iterrows():\n",
    "    df_metrics.loc[len(df_metrics)] = {'Word 1': row['Word 1'], 'Word 2': row['Word 2'], \"Human (mean)\": row['Human (mean)'], \"WU Palmer\": synsets_cycle(row['Word 1'], row['Word 2'], wu_palmer) * 10, \"Shortest Path\": (synsets_cycle(row['Word 1'], row['Word 2'], shortest_path) /40) * 10, \"Leakcock Chodorow\": (synsets_cycle(row['Word 1'], row['Word 2'], leakcock_chodorow) / math.log(41)) * 10}\n",
    "display(df_metrics)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation of correlation with Spearman and Pearson metrics between the human similarity and the similarity calculated by the three metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "WU Palmer Metric:\n",
      "Spearman Correlation: SignificanceResult(statistic=0.34531446776235897, pvalue=2.5376440594705738e-11)\n",
      "The distribution of the WU Palmer metric is significantly different from the distribution of the Human Similarity.\n",
      "\n",
      "Pearson Correlation: PearsonRResult(statistic=0.28732609798215053, pvalue=3.895408936985221e-08)\n",
      "The distribution of the WU Palmer metric is significantly different from the distribution of the Human Similarity.\n",
      "-------------------------------------------\n",
      "Shortest Path Metric:\n",
      "Spearman Correlation: SignificanceResult(statistic=0.2883010732370922, pvalue=3.4876313366885126e-08)\n",
      "The distribution of the Shortest Path metric is significantly different from the distribution of the Human Similarity.\n",
      "\n",
      "Pearson Correlation: PearsonRResult(statistic=0.16411938908972765, pvalue=0.0019775111177550756)\n",
      "The distribution of the Shortest Path metric is significantly different from the distribution of the Human Similarity.\n",
      "-------------------------------------------\n",
      "Leakcock Chodorow Metric:\n",
      "Spearman Correlation: SignificanceResult(statistic=0.2883010732370922, pvalue=3.4876313366885126e-08)\n",
      "The distribution of the Leakcock Chodorow metric is significantly different from the distribution of the Human Similarity.\n",
      "\n",
      "Pearson Correlation: PearsonRResult(statistic=0.317375647242182, pvalue=1.0592028515546679e-09)\n",
      "The distribution of the Leakcock Chodorow metric is significantly different from the distribution of the Human Similarity.\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Calculate the correlation between the human similarity and the similarity calculated by the three metrics\n",
    "\n",
    "print(\"-------------------------------------------\")\n",
    "print(\"WU Palmer Metric:\")\n",
    "spearman_correlation = st.spearmanr(df_metrics[\"WU Palmer\"], df_metrics[\"Human (mean)\"])\n",
    "print(\"Spearman Correlation:\", spearman_correlation)\n",
    "if spearman_correlation[1] < 0.05:\n",
    "    print(\"The distribution of the WU Palmer metric is significantly different from the distribution of the Human Similarity.\\n\")\n",
    "else:\n",
    "    print(\"The distribution of the WU Palmer metric is almost the same as the distribution of the Human Similarity.\\n\")\n",
    "pearson_correlation = st.pearsonr(df_metrics[\"WU Palmer\"], df_metrics[\"Human (mean)\"])\n",
    "print(\"Pearson Correlation:\", pearson_correlation)\n",
    "if pearson_correlation[1] < 0.05:\n",
    "    print(\"The distribution of the WU Palmer metric is significantly different from the distribution of the Human Similarity.\")\n",
    "else:\n",
    "    print(\"The distribution of the WU Palmer metric is almost the same as the distribution of the Human Similarity.\")\n",
    "print(\"-------------------------------------------\")\n",
    "\n",
    "print(\"Shortest Path Metric:\")\n",
    "spearman_correlation = st.spearmanr(df_metrics[\"Shortest Path\"], df_metrics[\"Human (mean)\"])\n",
    "print(\"Spearman Correlation:\", spearman_correlation)\n",
    "if spearman_correlation[1] < 0.05:\n",
    "    print(\"The distribution of the Shortest Path metric is significantly different from the distribution of the Human Similarity.\\n\")\n",
    "else:\n",
    "    print(\"The distribution of the Shortest Path metric is almost the same as the distribution of the Human Similarity.\\n\")\n",
    "pearson_correlation = st.pearsonr(df_metrics[\"Shortest Path\"], df_metrics[\"Human (mean)\"])\n",
    "print(\"Pearson Correlation:\", pearson_correlation)\n",
    "if pearson_correlation[1] < 0.05:\n",
    "    print(\"The distribution of the Shortest Path metric is significantly different from the distribution of the Human Similarity.\")\n",
    "else:\n",
    "    print(\"The distribution of the Shortest Path metric is almost the same as the distribution of the Human Similarity.\")\n",
    "print(\"-------------------------------------------\")\n",
    "\n",
    "print(\"Leakcock Chodorow Metric:\")\n",
    "spearman_correlation = st.spearmanr(df_metrics[\"Leakcock Chodorow\"], df_metrics[\"Human (mean)\"])\n",
    "print(\"Spearman Correlation:\", spearman_correlation)\n",
    "if spearman_correlation[1] < 0.05:\n",
    "    print(\"The distribution of the Leakcock Chodorow metric is significantly different from the distribution of the Human Similarity.\\n\")\n",
    "else:\n",
    "    print(\"The distribution of the Leakcock Chodorow metric is almost the same as the distribution of the Human Similarity.\\n\")\n",
    "pearson_correlation = st.pearsonr(df_metrics[\"Leakcock Chodorow\"], df_metrics[\"Human (mean)\"])\n",
    "print(\"Pearson Correlation:\", pearson_correlation)\n",
    "if pearson_correlation[1] < 0.05:\n",
    "    print(\"The distribution of the Leakcock Chodorow metric is significantly different from the distribution of the Human Similarity.\")\n",
    "else:\n",
    "    print(\"The distribution of the Leakcock Chodorow metric is almost the same as the distribution of the Human Similarity.\")\n",
    "print(\"-------------------------------------------\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Part\n",
    "\n",
    "- Implementing Lesk's algorithm (not from NLTK).\n",
    "    1. Extract 50 sentences from the SemCor corpus (corpus annotated with WN synsets) and disambiguate (at least) one noun per sentence. Calculate the accuracy of the implemented system based on the senses annotated in SemCor.\n",
    "    SemCor is available at the URL: http://web.eecs.umich.edu/~mihalcea/downloads.html\n",
    "    2. Randomize the selection of the 50 sentences and the selection of the term to be disambiguated, and return the average accuracy over (for example) 10 runs of the program.\n",
    "- [OPTIONAL]: implement corpus-Lesk algorithm using Sem-Cor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining support function for cleaning the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I clean the sentences removing the punctuation and the stop words\n",
    "\n",
    "def clean_sentence(sentence: str | list) -> list:\n",
    "    sentence_without_puntuaction = re.sub(r'[^\\w\\s]', '', str(sentence)).strip()\n",
    "    word_tokens = word_tokenize(sentence_without_puntuaction)\n",
    "    filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "    return filtered_sentence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Lesk Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_sentence = range(0,49)\n",
    "random_list_of_sentence = random.sample(range(0,len(semcor.sents())-1), 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I use the Lesk algorithm to find the best sense of the word in the sentence\n",
    "\n",
    "def lesk(word: str, sentence: list, clean: bool) -> Synset:\n",
    "    synsets = wn.synsets(word)\n",
    "    if not synsets:\n",
    "        return None\n",
    "    best_sense = synsets[0]\n",
    "    max_overlap = 0\n",
    "    for synset in synsets:\n",
    "        overlap = 0\n",
    "        # cleaning example in wordnet \n",
    "        for definition_word in clean_sentence(synset.definition()) if(clean) else synset.definition().split():\n",
    "            if definition_word in sentence:\n",
    "                overlap += 1\n",
    "        for example_word in clean_sentence(synset.examples()) if(clean) else synset.examples():\n",
    "            if(clean):\n",
    "                if example_word in sentence:\n",
    "                    overlap += 1\n",
    "            else:\n",
    "                for example_word in example_word.split():\n",
    "                    if example_word in sentence:\n",
    "                        overlap += 1\n",
    "        if overlap > max_overlap:\n",
    "            max_overlap = overlap\n",
    "            best_sense = synset\n",
    "        \n",
    "    return best_sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I select the sostantive words with pos_tag and I use the lesk algorithm to find the best synset\n",
    "# I use the synset of the semcor corpus to compare the results\n",
    "\n",
    "def get_accuracy_pos_tag(clean: bool, sentence_list: list) -> float:\n",
    "    points = 0\n",
    "    total = 0\n",
    "    for index in sentence_list:\n",
    "        sentence = semcor.sents()[index]\n",
    "        list_of_sentence_tagged = semcor.tagged_sents(tag='both')[index]\n",
    "        if clean:\n",
    "            #cleaning the phrase before pos tagging\n",
    "            sentence_tagged = nltk.pos_tag(clean_sentence(sentence))\n",
    "        else:\n",
    "            #the phrase remain dirty\n",
    "            sentence_tagged = nltk.pos_tag(word_tokenize(' '.join(sentence)))\n",
    "        for word, tag in sentence_tagged:\n",
    "            #only for noun\n",
    "            if tag == 'NN':\n",
    "                best_synset = lesk(word,clean_sentence(sentence) if clean else sentence,clean)\n",
    "                if best_synset is None:\n",
    "                    break\n",
    "                for tag in list_of_sentence_tagged:\n",
    "                    if tag.pos()[0][0] == word and tag.pos()[0][1] == 'NN' and hasattr(tag.label(), 'synset') and best_synset == tag.label().synset():\n",
    "                        points += 1\n",
    "                total += 1\n",
    "    print(\"Total number of words to disambiguate:\", total)\n",
    "    print(\"Number of words correctly disambiguated:\", points)\n",
    "    print(\"Accuracy:\", points/total)\n",
    "\n",
    "\n",
    "# I select the sostantive words directly from the semcor corpus and I use the lesk algorithm to to find the best synset.\n",
    "# I use the synset of the semcor corpus to compare the results\n",
    "\n",
    "def get_accuracy_from_corpus(clean: bool, sentence_list: list) -> float:\n",
    "    points = 0\n",
    "    total = 0\n",
    "    for index in sentence_list:\n",
    "        # number = 0\n",
    "        sentence = semcor.sents()[index]\n",
    "        list_of_sentence_tagged = semcor.tagged_sents(tag='both')[index]\n",
    "        for tag in list_of_sentence_tagged:\n",
    "            if hasattr(tag, 'label') and hasattr(tag.label(), 'synset'):\n",
    "                true_synset_word = tag.label().synset()\n",
    "                if true_synset_word.name().split('.')[1] == 'n':\n",
    "                    word_lemma = tag.label().name()\n",
    "                    best_synset = lesk(word_lemma,clean_sentence(sentence) if clean else sentence,clean)\n",
    "                    if best_synset is None:\n",
    "                        break\n",
    "                    if best_synset == true_synset_word:\n",
    "                        points += 1\n",
    "                    total += 1\n",
    "                    #number += 1\n",
    "            # if number >= 2:\n",
    "            #     break\n",
    "    print(\"Total number of words to disambiguate:\", total)\n",
    "    print(\"Number of words correctly disambiguated:\", points)\n",
    "    print(\"Accuracy:\", points/total )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Sostantive words extracted from the corpus\n",
      "\n",
      "Total number of words to disambiguate: 318\n",
      "Number of words correctly disambiguated: 207\n",
      "Accuracy: 0.6509433962264151\n",
      "-------------------------------------------\n",
      "Sostantive words extracted from the POS Tagging\n",
      "\n",
      "Total number of words to disambiguate: 191\n",
      "Number of words correctly disambiguated: 84\n",
      "Accuracy: 0.4397905759162304\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "Sostantive words extracted from the corpus without cleaning the sentences\n",
      "\n",
      "Total number of words to disambiguate: 318\n",
      "Number of words correctly disambiguated: 147\n",
      "Accuracy: 0.46226415094339623\n",
      "-------------------------------------------\n",
      "Sostantive words extracted from the POS Tagging without cleaning the sentences\n",
      "\n",
      "Total number of words to disambiguate: 195\n",
      "Number of words correctly disambiguated: 77\n",
      "Accuracy: 0.39487179487179486\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"-------------------------------------------\")\n",
    "print(\"Sostantive words extracted from the corpus\\n\")\n",
    "get_accuracy_from_corpus(True,list_of_sentence)\n",
    "print(\"-------------------------------------------\")\n",
    "print(\"Sostantive words extracted from the POS Tagging\\n\")\n",
    "get_accuracy_pos_tag(True,list_of_sentence)\n",
    "print(\"-------------------------------------------\")\n",
    "print(\"-------------------------------------------\")\n",
    "print(\"Sostantive words extracted from the corpus without cleaning the sentences\\n\")\n",
    "get_accuracy_from_corpus(False,list_of_sentence)\n",
    "print(\"-------------------------------------------\")\n",
    "print(\"Sostantive words extracted from the POS Tagging without cleaning the sentences\\n\")\n",
    "get_accuracy_pos_tag(False,list_of_sentence)\n",
    "print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Sostantive words extracted from the corpus\n",
      "\n",
      "Total number of words to disambiguate: 0\n",
      "Number of words correctly disambiguated: 0\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\gabri\\Desktop\\esercizi-radicioni\\es1\\exercise_1.ipynb Cell 33\u001b[0m in \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/gabri/Desktop/esercizi-radicioni/es1/exercise_1.ipynb#X44sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m-------------------------------------------\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/gabri/Desktop/esercizi-radicioni/es1/exercise_1.ipynb#X44sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mSostantive words extracted from the corpus\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/gabri/Desktop/esercizi-radicioni/es1/exercise_1.ipynb#X44sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m get_accuracy_from_corpus(\u001b[39mTrue\u001b[39;00m,[\u001b[39m32671\u001b[39m])\n",
      "\u001b[1;32mc:\\Users\\gabri\\Desktop\\esercizi-radicioni\\es1\\exercise_1.ipynb Cell 33\u001b[0m in \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gabri/Desktop/esercizi-radicioni/es1/exercise_1.ipynb#X44sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTotal number of words to disambiguate:\u001b[39m\u001b[39m\"\u001b[39m, total)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gabri/Desktop/esercizi-radicioni/es1/exercise_1.ipynb#X44sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mNumber of words correctly disambiguated:\u001b[39m\u001b[39m\"\u001b[39m, points)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/gabri/Desktop/esercizi-radicioni/es1/exercise_1.ipynb#X44sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mAccuracy:\u001b[39m\u001b[39m\"\u001b[39m, points\u001b[39m/\u001b[39;49mtotal )\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "print(\"-------------------------------------------\")\n",
    "print(\"Sostantive words extracted from the corpus\\n\")\n",
    "get_accuracy_from_corpus(True,random_list_of_sentence)\n",
    "print(\"-------------------------------------------\")\n",
    "print(\"Sostantive words extracted from the POS Tagging\\n\")\n",
    "get_accuracy_pos_tag(True,random_list_of_sentence)\n",
    "print(\"-------------------------------------------\")\n",
    "print(\"-------------------------------------------\")\n",
    "print(\"Sostantive words extracted from the corpus without cleaning the sentences\\n\")\n",
    "get_accuracy_from_corpus(False,random_list_of_sentence)\n",
    "print(\"-------------------------------------------\")\n",
    "print(\"Sostantive words extracted from the POS Tagging without cleaning the sentences\\n\")\n",
    "get_accuracy_pos_tag(False,random_list_of_sentence)\n",
    "print(\"-------------------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
