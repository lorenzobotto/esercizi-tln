{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Necessary imports and dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lores\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word 1</th>\n",
       "      <th>Word 2</th>\n",
       "      <th>Human (mean)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>love</td>\n",
       "      <td>sex</td>\n",
       "      <td>6.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tiger</td>\n",
       "      <td>cat</td>\n",
       "      <td>7.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tiger</td>\n",
       "      <td>tiger</td>\n",
       "      <td>10.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>book</td>\n",
       "      <td>paper</td>\n",
       "      <td>7.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>computer</td>\n",
       "      <td>keyboard</td>\n",
       "      <td>7.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>shower</td>\n",
       "      <td>flood</td>\n",
       "      <td>6.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>weather</td>\n",
       "      <td>forecast</td>\n",
       "      <td>8.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>disaster</td>\n",
       "      <td>area</td>\n",
       "      <td>6.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>governor</td>\n",
       "      <td>office</td>\n",
       "      <td>6.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>architecture</td>\n",
       "      <td>century</td>\n",
       "      <td>3.78</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>353 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Word 1    Word 2  Human (mean)\n",
       "0            love       sex          6.77\n",
       "1           tiger       cat          7.35\n",
       "2           tiger     tiger         10.00\n",
       "3            book     paper          7.46\n",
       "4        computer  keyboard          7.62\n",
       "..            ...       ...           ...\n",
       "348        shower     flood          6.03\n",
       "349       weather  forecast          8.34\n",
       "350      disaster      area          6.25\n",
       "351      governor    office          6.34\n",
       "352  architecture   century          3.78\n",
       "\n",
       "[353 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reading the file\n",
    "df = pd.read_csv('dataset\\combined.csv')\n",
    "display(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the function to get the max depth of the synset\n",
    "\n",
    "def max_depth_synset(synset):\n",
    "    return max(len(path) for path in synset.hypernym_paths())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the function to get the least common subsumer of two synsets\n",
    "\n",
    "def LCS(syn1,syn2):\n",
    "    path1 = syn1.hypernym_paths()\n",
    "    path2 = syn2.hypernym_paths()\n",
    "    result = []\n",
    "\n",
    "    # We loop through the paths of the first synset\n",
    "    for pat1 in path1:\n",
    "        x = pat1\n",
    "        x.reverse()\n",
    "        found = False\n",
    "        for syn in x:\n",
    "            if(not found):\n",
    "                # We loop through the paths of the second synset\n",
    "                for pat2 in path2:\n",
    "                    if(syn in pat2):\n",
    "                        result.append(syn)\n",
    "                        found = True\n",
    "                        break\n",
    "            else:\n",
    "                break\n",
    "    if not result:\n",
    "            return None\n",
    "    else:\n",
    "         max_depth = max(max_depth_synset(s) for s in result)\n",
    "         max_hypernym = max(s for s in result if max_depth_synset(s) == max_depth)\n",
    "         return max_hypernym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_len(syn1,syn2):\n",
    "    path1 = syn1.hypernym_paths()\n",
    "    path2 = syn2.hypernym_paths()\n",
    "    result_syn1 = []\n",
    "    result_syn2 = []\n",
    "\n",
    "    common_ancestor = LCS(syn1,syn2)\n",
    "\n",
    "    if(common_ancestor is None):\n",
    "        return None\n",
    "    else:\n",
    "        for pat1 in path1:\n",
    "            if(common_ancestor in pat1):\n",
    "                result_syn1.append(pat1.index(syn1)-pat1.index(common_ancestor))\n",
    "        for pat2 in path2:\n",
    "            if(common_ancestor in pat2):\n",
    "                result_syn2.append(pat2.index(syn2)-pat2.index(common_ancestor))\n",
    "        return min(result_syn1) + min(result_syn2)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_max = max(max(len(hyp_path) for hyp_path in ss.hypernym_paths()) for ss in wn.all_synsets())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wu Palmer\n",
    "\n",
    "$$ LCS(s1, s2) = {2 * depth(LCS(s1,s2)) \\over depth(s1) + depth(s2)} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wu_palmer(word1,word2):\n",
    "    # We get all the synsets of the two words\n",
    "    synw1 = wn.synsets(word1)\n",
    "    synw2 = wn.synsets(word2)\n",
    "    result = []\n",
    "    print(synw1)\n",
    "    print(synw2)\n",
    "    for syn1 in synw1:\n",
    "        depht1 = max_depth_synset(syn1)\n",
    "        for syn2 in synw2:\n",
    "            depht2 = max_depth_synset(syn2)\n",
    "            # Finding the least common subsumer of the two synsets\n",
    "            common_ancestor = LCS(syn1,syn2)\n",
    "            if(common_ancestor is None):\n",
    "                break\n",
    "            result.append( (2 * max_depth_synset(common_ancestor)) / ( depht1 + depht2 ) )\n",
    "            \n",
    "    return max(result)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[Synset('be.v.03'), Synset('populate.v.01')]]\n",
      "[[Synset('transfer.v.05'), Synset('give.v.03'), Synset('supply.v.01'), Synset('equip.v.01'), Synset('stock.v.02')]]\n",
      "[Synset('carnivore.n.01')]\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "# print(wn.synsets('stock'))\n",
    "# print(wn.synsets('live'))\n",
    "# print(wn.synset('live.v.01').wup_similarity(wn.synset('stock.v.01')))\n",
    "print(wn.synset('live.v.01').hypernym_paths())\n",
    "print(wn.synset('stock.v.02').hypernym_paths())\n",
    "print(wn.synset(\"canine.n.02\").hypernyms())\n",
    "\n",
    "#print(\"WU Palmer between dog and cat:\", wu_palmer('stock','live'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shortest Path\n",
    "\n",
    "$$ LCS(s1, s2) = 2 * depthMax - len(s1,s2) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shortest_path(word1,word2):\n",
    "    synw1 = wn.synsets(word1)\n",
    "    synw2 = wn.synsets(word2)\n",
    "    result = []\n",
    "\n",
    "    for syn1 in synw1:\n",
    "        for syn2 in synw2:\n",
    "            pat = path_len(syn1,syn2)\n",
    "            if pat is None:\n",
    "                break\n",
    "            elif (pat == 0):\n",
    "                result.append(2 * depth_max)\n",
    "            elif (pat == depth_max):\n",
    "                result.append(0)\n",
    "            else:\n",
    "                result.append(2 * depth_max - pat)\n",
    "    return max(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shortest Path between dog and cat: 24\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "\n",
    "print(\"Shortest Path between dog and cat:\", shortest_path('Jerusalem','Palestinian'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leakcock & Chodorow\n",
    "\n",
    "$$ sim_LC(s1, s2) = -log(\\frac{len(s1,s2)}{2*depthMax}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leakcock_chodorow (word1,word2):\n",
    "    synw1 = wn.synsets(word1)\n",
    "    synw2 = wn.synsets(word2)\n",
    "    result = []\n",
    "\n",
    "    for syn1 in synw1:\n",
    "        for syn2 in synw2:\n",
    "            pat = path_len(syn1,syn2)\n",
    "            if pat is None:\n",
    "                break\n",
    "            elif (pat == 0):\n",
    "                result.append( -math.log( (1)/(2 * depth_max + 1) ))\n",
    "            else:\n",
    "                result.append(-math.log( (pat)/(2 * depth_max) ))\n",
    "    return max(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shortest Path between dog and cat: 2.3025850929940455\n",
      "lch_similarity 2.0281482472922856\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "\n",
    "print(\"Shortest Path between dog and cat:\", leakcock_chodorow('dog','cat'))\n",
    "print(\"lch_similarity\", wn.synset('dog.n.01').lch_similarity(wn.synset('cat.n.01')))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop over all pair of words in the dataset and i print the three similarity measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Word 1: love\n",
      "Word 2: sex\n",
      "Human Similarity: 6.77\n",
      "[Synset('love.n.01'), Synset('love.n.02'), Synset('beloved.n.01'), Synset('love.n.04'), Synset('love.n.05'), Synset('sexual_love.n.02'), Synset('love.v.01'), Synset('love.v.02'), Synset('love.v.03'), Synset('sleep_together.v.01')]\n",
      "[Synset('sexual_activity.n.01'), Synset('sex.n.02'), Synset('sex.n.03'), Synset('sex.n.04'), Synset('arouse.v.07'), Synset('sex.v.02')]\n",
      "WU Palmer: 0.9230769230769231\n",
      "Shortest Path: 39\n",
      "Leacock Chodorow: 3.6888794541139363\n",
      "--------------------------------------\n",
      "Word 1: tiger\n",
      "Word 2: cat\n",
      "Human Similarity: 7.35\n",
      "[Synset('tiger.n.01'), Synset('tiger.n.02')]\n",
      "[Synset('cat.n.01'), Synset('guy.n.01'), Synset('cat.n.03'), Synset('kat.n.01'), Synset('cat-o'-nine-tails.n.01'), Synset('caterpillar.n.02'), Synset('big_cat.n.01'), Synset('computerized_tomography.n.01'), Synset('cat.v.01'), Synset('vomit.v.01')]\n",
      "WU Palmer: 0.9655172413793104\n",
      "Shortest Path: 39\n",
      "Leacock Chodorow: 3.6888794541139363\n",
      "--------------------------------------\n",
      "Word 1: tiger\n",
      "Word 2: tiger\n",
      "Human Similarity: 10.0\n",
      "[Synset('tiger.n.01'), Synset('tiger.n.02')]\n",
      "[Synset('tiger.n.01'), Synset('tiger.n.02')]\n",
      "WU Palmer: 1.0\n",
      "Shortest Path: 40\n",
      "Leacock Chodorow: 3.713572066704308\n",
      "--------------------------------------\n",
      "Word 1: book\n",
      "Word 2: paper\n",
      "Human Similarity: 7.46\n",
      "[Synset('book.n.01'), Synset('book.n.02'), Synset('record.n.05'), Synset('script.n.01'), Synset('ledger.n.01'), Synset('book.n.06'), Synset('book.n.07'), Synset('koran.n.01'), Synset('bible.n.01'), Synset('book.n.10'), Synset('book.n.11'), Synset('book.v.01'), Synset('reserve.v.04'), Synset('book.v.03'), Synset('book.v.04')]\n",
      "[Synset('paper.n.01'), Synset('composition.n.08'), Synset('newspaper.n.01'), Synset('paper.n.04'), Synset('paper.n.05'), Synset('newspaper.n.02'), Synset('newspaper.n.03'), Synset('paper.v.01'), Synset('wallpaper.v.01')]\n",
      "WU Palmer: 0.875\n",
      "Shortest Path: 38\n",
      "Leacock Chodorow: 2.995732273553991\n",
      "--------------------------------------\n",
      "Word 1: computer\n",
      "Word 2: keyboard\n",
      "Human Similarity: 7.62\n",
      "[Synset('computer.n.01'), Synset('calculator.n.01')]\n",
      "[Synset('keyboard.n.01'), Synset('keyboard.n.02')]\n",
      "WU Palmer: 0.8235294117647058\n",
      "Shortest Path: 37\n",
      "Leacock Chodorow: 2.5902671654458267\n",
      "--------------------------------------\n",
      "Word 1: computer\n",
      "Word 2: internet\n",
      "Human Similarity: 7.58\n",
      "[Synset('computer.n.01'), Synset('calculator.n.01')]\n",
      "[Synset('internet.n.01')]\n",
      "WU Palmer: 0.631578947368421\n",
      "Shortest Path: 33\n",
      "Leacock Chodorow: 1.742969305058623\n",
      "--------------------------------------\n",
      "Word 1: plane\n",
      "Word 2: car\n",
      "Human Similarity: 5.77\n",
      "[Synset('airplane.n.01'), Synset('plane.n.02'), Synset('plane.n.03'), Synset('plane.n.04'), Synset('plane.n.05'), Synset('plane.v.01'), Synset('plane.v.02'), Synset('plane.v.03'), Synset('flat.s.01')]\n",
      "[Synset('car.n.01'), Synset('car.n.02'), Synset('car.n.03'), Synset('car.n.04'), Synset('cable_car.n.01')]\n",
      "WU Palmer: 0.7272727272727273\n",
      "Shortest Path: 34\n",
      "Leacock Chodorow: 1.8971199848858813\n",
      "--------------------------------------\n",
      "Word 1: train\n",
      "Word 2: car\n",
      "Human Similarity: 6.31\n",
      "[Synset('train.n.01'), Synset('string.n.04'), Synset('caravan.n.01'), Synset('train.n.04'), Synset('train.n.05'), Synset('gearing.n.01'), Synset('train.v.01'), Synset('train.v.02'), Synset('discipline.v.01'), Synset('prepare.v.05'), Synset('educate.v.03'), Synset('aim.v.01'), Synset('coach.v.01'), Synset('train.v.08'), Synset('train.v.09'), Synset('train.v.10'), Synset('trail.v.05')]\n",
      "[Synset('car.n.01'), Synset('car.n.02'), Synset('car.n.03'), Synset('car.n.04'), Synset('cable_car.n.01')]\n",
      "WU Palmer: 0.7368421052631579\n",
      "Shortest Path: 35\n",
      "Leacock Chodorow: 2.0794415416798357\n",
      "--------------------------------------\n",
      "Word 1: telephone\n",
      "Word 2: communication\n",
      "Human Similarity: 7.5\n",
      "[Synset('telephone.n.01'), Synset('telephone.n.02'), Synset('call.v.03')]\n",
      "[Synset('communication.n.01'), Synset('communication.n.02'), Synset('communication.n.03')]\n",
      "WU Palmer: 0.16666666666666666\n",
      "Shortest Path: 30\n",
      "Leacock Chodorow: 1.3862943611198906\n",
      "--------------------------------------\n",
      "Word 1: television\n",
      "Word 2: radio\n",
      "Human Similarity: 6.77\n",
      "[Synset('television.n.01'), Synset('television.n.02'), Synset('television_receiver.n.01')]\n",
      "[Synset('radio.n.01'), Synset('radio_receiver.n.01'), Synset('radio.n.03'), Synset('radio.v.01'), Synset('radio.a.01')]\n",
      "WU Palmer: 0.9090909090909091\n",
      "Shortest Path: 38\n",
      "Leacock Chodorow: 2.995732273553991\n",
      "--------------------------------------\n",
      "Word 1: media\n",
      "Word 2: radio\n",
      "Human Similarity: 7.42\n",
      "[Synset('medium.n.01'), Synset('medium.n.02'), Synset('medium.n.03'), Synset('culture_medium.n.01'), Synset('medium.n.05'), Synset('medium.n.06'), Synset('medium.n.07'), Synset('medium.n.08'), Synset('medium.n.09'), Synset('medium.n.10'), Synset('metier.n.02')]\n",
      "[Synset('radio.n.01'), Synset('radio_receiver.n.01'), Synset('radio.n.03'), Synset('radio.v.01'), Synset('radio.a.01')]\n",
      "WU Palmer: 0.8235294117647058\n",
      "Shortest Path: 37\n",
      "Leacock Chodorow: 2.5902671654458267\n",
      "--------------------------------------\n",
      "Word 1: drug\n",
      "Word 2: abuse\n",
      "Human Similarity: 6.85\n",
      "[Synset('drug.n.01'), Synset('drug.v.01'), Synset('drug.v.02')]\n",
      "[Synset('maltreatment.n.01'), Synset('abuse.n.02'), Synset('misuse.n.01'), Synset('mistreat.v.01'), Synset('pervert.v.03'), Synset('abuse.v.03'), Synset('abuse.v.04')]\n",
      "WU Palmer: 0.16666666666666666\n",
      "Shortest Path: 31\n",
      "Leacock Chodorow: 1.491654876777717\n",
      "--------------------------------------\n",
      "Word 1: bread\n",
      "Word 2: butter\n",
      "Human Similarity: 6.19\n",
      "[Synset('bread.n.01'), Synset('boodle.n.01'), Synset('bread.v.01')]\n",
      "[Synset('butter.n.01'), Synset('butter.n.02'), Synset('butter.v.01')]\n",
      "WU Palmer: 0.75\n",
      "Shortest Path: 36\n",
      "Leacock Chodorow: 2.3025850929940455\n",
      "--------------------------------------\n",
      "Word 1: cucumber\n",
      "Word 2: potato\n",
      "Human Similarity: 5.92\n",
      "[Synset('cucumber.n.01'), Synset('cucumber.n.02')]\n",
      "[Synset('potato.n.01'), Synset('potato.n.02')]\n",
      "WU Palmer: 0.8235294117647058\n",
      "Shortest Path: 37\n",
      "Leacock Chodorow: 2.5902671654458267\n",
      "--------------------------------------\n",
      "Word 1: doctor\n",
      "Word 2: nurse\n",
      "Human Similarity: 7.0\n",
      "[Synset('doctor.n.01'), Synset('doctor_of_the_church.n.01'), Synset('doctor.n.03'), Synset('doctor.n.04'), Synset('sophisticate.v.03'), Synset('doctor.v.02'), Synset('repair.v.01')]\n",
      "[Synset('nurse.n.01'), Synset('nanny.n.01'), Synset('nurse.v.01'), Synset('harbor.v.01'), Synset('nurse.v.03'), Synset('nurse.v.04'), Synset('breastfeed.v.01')]\n",
      "WU Palmer: 0.8695652173913043\n",
      "Shortest Path: 37\n",
      "Leacock Chodorow: 2.5902671654458267\n",
      "--------------------------------------\n",
      "Word 1: professor\n",
      "Word 2: doctor\n",
      "Human Similarity: 6.62\n",
      "[Synset('professor.n.01')]\n",
      "[Synset('doctor.n.01'), Synset('doctor_of_the_church.n.01'), Synset('doctor.n.03'), Synset('doctor.n.04'), Synset('sophisticate.v.03'), Synset('doctor.v.02'), Synset('repair.v.01')]\n",
      "WU Palmer: 0.75\n",
      "Shortest Path: 34\n",
      "Leacock Chodorow: 1.8971199848858813\n",
      "--------------------------------------\n",
      "Word 1: student\n",
      "Word 2: professor\n",
      "Human Similarity: 6.81\n",
      "[Synset('student.n.01'), Synset('scholar.n.01')]\n",
      "[Synset('professor.n.01')]\n",
      "WU Palmer: 0.6666666666666666\n",
      "Shortest Path: 33\n",
      "Leacock Chodorow: 1.742969305058623\n",
      "--------------------------------------\n",
      "Word 1: smart\n",
      "Word 2: student\n",
      "Human Similarity: 4.62\n",
      "[Synset('smart.n.01'), Synset('ache.v.03'), Synset('smart.a.01'), Synset('chic.s.01'), Synset('bright.s.03'), Synset('fresh.s.12'), Synset('smart.s.05'), Synset('smart.s.06'), Synset('smart.s.07')]\n",
      "[Synset('student.n.01'), Synset('scholar.n.01')]\n",
      "WU Palmer: 0.1111111111111111\n",
      "Shortest Path: 27\n",
      "Leacock Chodorow: 1.1239300966523995\n",
      "--------------------------------------\n",
      "Word 1: smart\n",
      "Word 2: stupid\n",
      "Human Similarity: 5.81\n",
      "[Synset('smart.n.01'), Synset('ache.v.03'), Synset('smart.a.01'), Synset('chic.s.01'), Synset('bright.s.03'), Synset('fresh.s.12'), Synset('smart.s.05'), Synset('smart.s.06'), Synset('smart.s.07')]\n",
      "[Synset('stupid.n.01'), Synset('stupid.a.01'), Synset('dazed.s.01'), Synset('unintelligent.a.01')]\n",
      "WU Palmer: 0.1111111111111111\n",
      "Shortest Path: 27\n",
      "Leacock Chodorow: 1.1239300966523995\n",
      "--------------------------------------\n",
      "Word 1: company\n",
      "Word 2: stock\n",
      "Human Similarity: 7.08\n",
      "[Synset('company.n.01'), Synset('company.n.02'), Synset('company.n.03'), Synset('company.n.04'), Synset('caller.n.01'), Synset('company.n.06'), Synset('party.n.03'), Synset('ship's_company.n.01'), Synset('company.n.09'), Synset('company.v.01')]\n",
      "[Synset('stock.n.01'), Synset('stock.n.02'), Synset('stock.n.03'), Synset('stock_certificate.n.01'), Synset('store.n.02'), Synset('lineage.n.01'), Synset('breed.n.01'), Synset('broth.n.01'), Synset('stock.n.09'), Synset('stock.n.10'), Synset('stock.n.11'), Synset('stock.n.12'), Synset('malcolm_stock.n.01'), Synset('stock.n.14'), Synset('stock.n.15'), Synset('neckcloth.n.01'), Synset('livestock.n.01'), Synset('stock.v.01'), Synset('stock.v.02'), Synset('stock.v.03'), Synset('stock.v.04'), Synset('stock.v.05'), Synset('stock.v.06'), Synset('sprout.v.02'), Synset('banal.s.01'), Synset('stock.s.02'), Synset('standard.s.05')]\n",
      "WU Palmer: 0.6153846153846154\n",
      "Shortest Path: 35\n",
      "Leacock Chodorow: 2.0794415416798357\n",
      "--------------------------------------\n",
      "Word 1: stock\n",
      "Word 2: market\n",
      "Human Similarity: 8.08\n",
      "[Synset('stock.n.01'), Synset('stock.n.02'), Synset('stock.n.03'), Synset('stock_certificate.n.01'), Synset('store.n.02'), Synset('lineage.n.01'), Synset('breed.n.01'), Synset('broth.n.01'), Synset('stock.n.09'), Synset('stock.n.10'), Synset('stock.n.11'), Synset('stock.n.12'), Synset('malcolm_stock.n.01'), Synset('stock.n.14'), Synset('stock.n.15'), Synset('neckcloth.n.01'), Synset('livestock.n.01'), Synset('stock.v.01'), Synset('stock.v.02'), Synset('stock.v.03'), Synset('stock.v.04'), Synset('stock.v.05'), Synset('stock.v.06'), Synset('sprout.v.02'), Synset('banal.s.01'), Synset('stock.s.02'), Synset('standard.s.05')]\n",
      "[Synset('market.n.01'), Synset('market.n.02'), Synset('grocery_store.n.01'), Synset('market.n.04'), Synset('marketplace.n.02'), Synset('market.v.01'), Synset('market.v.02'), Synset('market.v.03'), Synset('commercialize.v.02')]\n",
      "WU Palmer: 0.5555555555555556\n",
      "Shortest Path: 34\n",
      "Leacock Chodorow: 1.8971199848858813\n",
      "--------------------------------------\n",
      "Word 1: stock\n",
      "Word 2: phone\n",
      "Human Similarity: 1.62\n",
      "[Synset('stock.n.01'), Synset('stock.n.02'), Synset('stock.n.03'), Synset('stock_certificate.n.01'), Synset('store.n.02'), Synset('lineage.n.01'), Synset('breed.n.01'), Synset('broth.n.01'), Synset('stock.n.09'), Synset('stock.n.10'), Synset('stock.n.11'), Synset('stock.n.12'), Synset('malcolm_stock.n.01'), Synset('stock.n.14'), Synset('stock.n.15'), Synset('neckcloth.n.01'), Synset('livestock.n.01'), Synset('stock.v.01'), Synset('stock.v.02'), Synset('stock.v.03'), Synset('stock.v.04'), Synset('stock.v.05'), Synset('stock.v.06'), Synset('sprout.v.02'), Synset('banal.s.01'), Synset('stock.s.02'), Synset('standard.s.05')]\n",
      "[Synset('telephone.n.01'), Synset('phone.n.02'), Synset('earphone.n.01'), Synset('call.v.03')]\n",
      "WU Palmer: 0.7\n",
      "Shortest Path: 34\n",
      "Leacock Chodorow: 1.8971199848858813\n",
      "--------------------------------------\n",
      "Word 1: stock\n",
      "Word 2: CD\n",
      "Human Similarity: 1.31\n",
      "[Synset('stock.n.01'), Synset('stock.n.02'), Synset('stock.n.03'), Synset('stock_certificate.n.01'), Synset('store.n.02'), Synset('lineage.n.01'), Synset('breed.n.01'), Synset('broth.n.01'), Synset('stock.n.09'), Synset('stock.n.10'), Synset('stock.n.11'), Synset('stock.n.12'), Synset('malcolm_stock.n.01'), Synset('stock.n.14'), Synset('stock.n.15'), Synset('neckcloth.n.01'), Synset('livestock.n.01'), Synset('stock.v.01'), Synset('stock.v.02'), Synset('stock.v.03'), Synset('stock.v.04'), Synset('stock.v.05'), Synset('stock.v.06'), Synset('sprout.v.02'), Synset('banal.s.01'), Synset('stock.s.02'), Synset('standard.s.05')]\n",
      "[Synset('cadmium.n.01'), Synset('candle.n.02'), Synset('certificate_of_deposit.n.01'), Synset('compact_disk.n.01'), Synset('four_hundred.s.01')]\n",
      "WU Palmer: 0.7368421052631579\n",
      "Shortest Path: 35\n",
      "Leacock Chodorow: 2.0794415416798357\n",
      "--------------------------------------\n",
      "Word 1: stock\n",
      "Word 2: jaguar\n",
      "Human Similarity: 0.92\n",
      "[Synset('stock.n.01'), Synset('stock.n.02'), Synset('stock.n.03'), Synset('stock_certificate.n.01'), Synset('store.n.02'), Synset('lineage.n.01'), Synset('breed.n.01'), Synset('broth.n.01'), Synset('stock.n.09'), Synset('stock.n.10'), Synset('stock.n.11'), Synset('stock.n.12'), Synset('malcolm_stock.n.01'), Synset('stock.n.14'), Synset('stock.n.15'), Synset('neckcloth.n.01'), Synset('livestock.n.01'), Synset('stock.v.01'), Synset('stock.v.02'), Synset('stock.v.03'), Synset('stock.v.04'), Synset('stock.v.05'), Synset('stock.v.06'), Synset('sprout.v.02'), Synset('banal.s.01'), Synset('stock.s.02'), Synset('standard.s.05')]\n",
      "[Synset('jaguar.n.01')]\n",
      "WU Palmer: 0.8148148148148148\n",
      "Shortest Path: 35\n",
      "Leacock Chodorow: 2.0794415416798357\n",
      "--------------------------------------\n",
      "Word 1: stock\n",
      "Word 2: egg\n",
      "Human Similarity: 1.81\n",
      "[Synset('stock.n.01'), Synset('stock.n.02'), Synset('stock.n.03'), Synset('stock_certificate.n.01'), Synset('store.n.02'), Synset('lineage.n.01'), Synset('breed.n.01'), Synset('broth.n.01'), Synset('stock.n.09'), Synset('stock.n.10'), Synset('stock.n.11'), Synset('stock.n.12'), Synset('malcolm_stock.n.01'), Synset('stock.n.14'), Synset('stock.n.15'), Synset('neckcloth.n.01'), Synset('livestock.n.01'), Synset('stock.v.01'), Synset('stock.v.02'), Synset('stock.v.03'), Synset('stock.v.04'), Synset('stock.v.05'), Synset('stock.v.06'), Synset('sprout.v.02'), Synset('banal.s.01'), Synset('stock.s.02'), Synset('standard.s.05')]\n",
      "[Synset('egg.n.01'), Synset('egg.n.02'), Synset('testis.n.01'), Synset('egg.v.01'), Synset('egg.v.02')]\n",
      "WU Palmer: 0.625\n",
      "Shortest Path: 34\n",
      "Leacock Chodorow: 1.8971199848858813\n",
      "--------------------------------------\n",
      "Word 1: fertility\n",
      "Word 2: egg\n",
      "Human Similarity: 6.69\n",
      "[Synset('birthrate.n.01'), Synset('fertility.n.02'), Synset('richness.n.05')]\n",
      "[Synset('egg.n.01'), Synset('egg.n.02'), Synset('testis.n.01'), Synset('egg.v.01'), Synset('egg.v.02')]\n",
      "WU Palmer: 0.15384615384615385\n",
      "Shortest Path: 29\n",
      "Leacock Chodorow: 1.2909841813155656\n",
      "--------------------------------------\n",
      "Word 1: stock\n",
      "Word 2: live\n",
      "Human Similarity: 3.73\n",
      "[Synset('stock.n.01'), Synset('stock.n.02'), Synset('stock.n.03'), Synset('stock_certificate.n.01'), Synset('store.n.02'), Synset('lineage.n.01'), Synset('breed.n.01'), Synset('broth.n.01'), Synset('stock.n.09'), Synset('stock.n.10'), Synset('stock.n.11'), Synset('stock.n.12'), Synset('malcolm_stock.n.01'), Synset('stock.n.14'), Synset('stock.n.15'), Synset('neckcloth.n.01'), Synset('livestock.n.01'), Synset('stock.v.01'), Synset('stock.v.02'), Synset('stock.v.03'), Synset('stock.v.04'), Synset('stock.v.05'), Synset('stock.v.06'), Synset('sprout.v.02'), Synset('banal.s.01'), Synset('stock.s.02'), Synset('standard.s.05')]\n",
      "[Synset('populate.v.01'), Synset('live.v.02'), Synset('survive.v.01'), Synset('exist.v.02'), Synset('be.v.11'), Synset('know.v.05'), Synset('live.v.07'), Synset('live.a.01'), Synset('live.a.02'), Synset('alive.a.01'), Synset('live.s.04'), Synset('live.s.05'), Synset('bouncy.s.01'), Synset('live.s.07'), Synset('live.s.08'), Synset('live.s.09'), Synset('hot.s.20'), Synset('alive.s.07'), Synset('live.r.01')]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "max() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[127], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mWord 2:\u001b[39m\u001b[39m\"\u001b[39m, row[\u001b[39m'\u001b[39m\u001b[39mWord 2\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m      6\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mHuman Similarity:\u001b[39m\u001b[39m\"\u001b[39m, row[\u001b[39m'\u001b[39m\u001b[39mHuman (mean)\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m----> 7\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mWU Palmer:\u001b[39m\u001b[39m\"\u001b[39m, wu_palmer(row[\u001b[39m'\u001b[39;49m\u001b[39mWord 1\u001b[39;49m\u001b[39m'\u001b[39;49m], row[\u001b[39m'\u001b[39;49m\u001b[39mWord 2\u001b[39;49m\u001b[39m'\u001b[39;49m]))\n\u001b[0;32m      8\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mShortest Path:\u001b[39m\u001b[39m\"\u001b[39m, shortest_path(row[\u001b[39m'\u001b[39m\u001b[39mWord 1\u001b[39m\u001b[39m'\u001b[39m], row[\u001b[39m'\u001b[39m\u001b[39mWord 2\u001b[39m\u001b[39m'\u001b[39m]))\n\u001b[0;32m      9\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mLeacock Chodorow:\u001b[39m\u001b[39m\"\u001b[39m, leakcock_chodorow(row[\u001b[39m'\u001b[39m\u001b[39mWord 1\u001b[39m\u001b[39m'\u001b[39m], row[\u001b[39m'\u001b[39m\u001b[39mWord 2\u001b[39m\u001b[39m'\u001b[39m]))\n",
      "Cell \u001b[1;32mIn[100], line 18\u001b[0m, in \u001b[0;36mwu_palmer\u001b[1;34m(word1, word2)\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m     16\u001b[0m         result\u001b[39m.\u001b[39mappend( (\u001b[39m2\u001b[39m \u001b[39m*\u001b[39m max_depth_synset(common_ancestor)) \u001b[39m/\u001b[39m ( depht1 \u001b[39m+\u001b[39m depht2 ) )\n\u001b[1;32m---> 18\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mmax\u001b[39;49m(result)\n",
      "\u001b[1;31mValueError\u001b[0m: max() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "# Loop through the dataset and calculate the similarity for each pair of words\n",
    "for index, row in df.iterrows():\n",
    "    print(\"--------------------------------------\")\n",
    "    print(\"Word 1:\", row['Word 1'])\n",
    "    print(\"Word 2:\", row['Word 2'])\n",
    "    print(\"Human Similarity:\", row['Human (mean)'])\n",
    "    print(\"WU Palmer:\", wu_palmer(row['Word 1'], row['Word 2']))\n",
    "    print(\"Shortest Path:\", shortest_path(row['Word 1'], row['Word 2']))\n",
    "    print(\"Leacock Chodorow:\", leakcock_chodorow(row['Word 1'], row['Word 2']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
