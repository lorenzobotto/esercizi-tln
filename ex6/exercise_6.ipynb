{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6 - Part C"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import multiprocessing\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import utils\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.utils import simple_preprocess\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "tqdm.pandas(desc=\"progress-bar\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the structure of pandas dataframe\n",
    "df = pd.DataFrame(columns=[\"synset\", \"synonyms\", \"definition\", \"target\"])\n",
    "\n",
    "for filename in os.listdir(\"data/\"):\n",
    "    with open(os.path.join(\"data/\", filename), 'r') as f:\n",
    "        if filename == \"1.json\":\n",
    "            data = json.loads(f.read())\n",
    "            synsets = data[\"dataset\"]\n",
    "            for index, synset in enumerate(synsets):\n",
    "                df.loc[len(df)] = [\n",
    "                    synset.split(\":\")[0],\n",
    "                    synset.split(\":\")[1].split(\"|\")[0].strip(),\n",
    "                    synset.split(\":\")[2].strip(),\n",
    "                    data[\"answers\"][index]\n",
    "                ]\n",
    "        else:\n",
    "            data = json.loads(f.read())\n",
    "            answers = data[\"answers\"]\n",
    "            for index, answer in enumerate(answers):\n",
    "                df.at[index, 'target'] = df.iloc[index]['target'] + \", \" + answer\n",
    "\n",
    "# Produce an unique target label that is the common one\n",
    "df['target'] = df['target'].apply(lambda x: max(set(x.split(\", \")), key=x.split(\", \").count))\n",
    "df['target'] = df['target'].map({'basic': 0, 'advanced': 1})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing of definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim pre-process (tokenize, remove stopwords, tokenize)\n",
    "df['definition'] = df['definition'].apply(simple_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['definition'], df['target'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Creating the tagged documents for the Doc2Vec model\n",
    "train_tagged = []\n",
    "for index, sentence in enumerate(X_train):\n",
    "    train_tagged.append(TaggedDocument(sentence, [y_train.iloc[index]]))\n",
    "\n",
    "test_tagged = []\n",
    "for index, sentence in enumerate(X_test):\n",
    "    test_tagged.append(TaggedDocument(sentence, [y_test.iloc[index]]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc2vec model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 403/403 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# Creating the Doc2Vec model\n",
    "cores = multiprocessing.cpu_count()\n",
    "model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, workers=cores)\n",
    "model_dbow.build_vocab([x for x in tqdm(train_tagged)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 403/403 [00:00<00:00, 118567.94it/s]\n",
      "100%|██████████| 403/403 [00:00<?, ?it/s]\n",
      "100%|██████████| 403/403 [00:00<00:00, 49211.15it/s]\n",
      "100%|██████████| 403/403 [00:00<?, ?it/s]\n",
      "100%|██████████| 403/403 [00:00<?, ?it/s]\n",
      "100%|██████████| 403/403 [00:00<?, ?it/s]\n",
      "100%|██████████| 403/403 [00:00<00:00, 49973.53it/s]\n",
      "100%|██████████| 403/403 [00:00<?, ?it/s]\n",
      "100%|██████████| 403/403 [00:00<?, ?it/s]\n",
      "100%|██████████| 403/403 [00:00<?, ?it/s]\n",
      "100%|██████████| 403/403 [00:00<?, ?it/s]\n",
      "100%|██████████| 403/403 [00:00<?, ?it/s]\n",
      "100%|██████████| 403/403 [00:00<?, ?it/s]\n",
      "100%|██████████| 403/403 [00:00<?, ?it/s]\n",
      "100%|██████████| 403/403 [00:00<?, ?it/s]\n",
      "100%|██████████| 403/403 [00:00<?, ?it/s]\n",
      "100%|██████████| 403/403 [00:00<?, ?it/s]\n",
      "100%|██████████| 403/403 [00:00<?, ?it/s]\n",
      "100%|██████████| 403/403 [00:00<?, ?it/s]\n",
      "100%|██████████| 403/403 [00:00<?, ?it/s]\n",
      "100%|██████████| 403/403 [00:00<00:00, 106081.62it/s]\n",
      "100%|██████████| 403/403 [00:00<00:00, 90671.84it/s]\n",
      "100%|██████████| 403/403 [00:00<?, ?it/s]\n",
      "100%|██████████| 403/403 [00:00<?, ?it/s]\n",
      "100%|██████████| 403/403 [00:00<?, ?it/s]\n",
      "100%|██████████| 403/403 [00:00<00:00, 343837.37it/s]\n",
      "100%|██████████| 403/403 [00:00<?, ?it/s]\n",
      "100%|██████████| 403/403 [00:00<?, ?it/s]\n",
      "100%|██████████| 403/403 [00:00<?, ?it/s]\n",
      "100%|██████████| 403/403 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# Training the Doc2Vec model\n",
    "for epoch in range(30):\n",
    "    model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged)]), total_examples=len(train_tagged), epochs=1)\n",
    "    model_dbow.alpha -= 0.002\n",
    "    model_dbow.min_alpha = model_dbow.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_for_learning(model, tagged_docs):\n",
    "    sents = tagged_docs\n",
    "    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, epochs=20)) for doc in sents])\n",
    "    return targets, regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy 0.5445544554455446\n",
      "Testing F1 score: 0.5461746174617462\n"
     ]
    }
   ],
   "source": [
    "# Training the Logistic Regression model\n",
    "y_train, X_train = vec_for_learning(model_dbow, train_tagged)\n",
    "y_test, X_test = vec_for_learning(model_dbow, test_tagged)\n",
    "logreg = LogisticRegression(n_jobs=1, C=1e5, max_iter=2000)\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation mean score - Training Set: 0.5007926829268292\n"
     ]
    }
   ],
   "source": [
    "# 10-fold cross-validation on training set\n",
    "scores = cross_val_score(logreg, X_train, y_train, cv=10)\n",
    "print(\"Cross-validation mean score - Training Set: {}\".format(scores.mean()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
