{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6 - Part C"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import multiprocessing\n",
    "import random\n",
    "from sklearn import utils\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.utils import simple_preprocess\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Embedding\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the structure of pandas dataframe\n",
    "def load_dataset():\n",
    "    df = pd.DataFrame(columns=[\"synset\", \"synonyms\", \"definition\", \"target\", \"timeDiffs\", \"isHard\"])\n",
    "\n",
    "    for filename in os.listdir(\"data/\"):\n",
    "        with open(os.path.join(\"data/\", filename), 'r') as f:\n",
    "            if filename == \"1.json\":\n",
    "                data = json.loads(f.read())\n",
    "                synsets = data[\"dataset\"]\n",
    "                for index, synset in enumerate(synsets):\n",
    "                    df.loc[len(df)] = [\n",
    "                        synset.split(\":\")[0],\n",
    "                        synset.split(\":\")[1].split(\"|\")[0].strip(),\n",
    "                        synset.split(\":\")[2].strip(),\n",
    "                        data[\"answers\"][index],\n",
    "                        data[\"timeDiffs\"][index],\n",
    "                        data[\"isHard\"][index]\n",
    "                    ]\n",
    "            elif filename != \"glove.6B.100d.txt\":\n",
    "                data = json.loads(f.read())\n",
    "                answers = data[\"answers\"]\n",
    "                for index, answer in enumerate(answers):\n",
    "                    df.at[index, 'target'] = df.iloc[index]['target'] + \", \" + answer\n",
    "                    df.at[index, 'timeDiffs'] = str(df.iloc[index]['timeDiffs']) + \", \" + str(data[\"timeDiffs\"][index])\n",
    "                    df.at[index, 'isHard'] = str(df.iloc[index]['isHard']) + \", \" + str(data[\"isHard\"][index])\n",
    "\n",
    "    # Produce an unique target label and isHard that is the common one\n",
    "    df['target'] = df['target'].apply(lambda x: max(set(x.split(\", \")), key=x.split(\", \").count))\n",
    "    df['target'] = df['target'].map({'basic': 0, 'advanced': 1})\n",
    "    df['isHard'] = df['isHard'].apply(lambda x: max(set(x.split(\", \")), key=x.split(\", \").count))\n",
    "    df['isHard'] = df['isHard'].map({'False': 0, 'True': 1})\n",
    "\n",
    "    # Avarage timeDiffs for each synset\n",
    "    df['timeDiffs'] = df['timeDiffs'].apply(lambda x: np.mean([float(i) for i in x.split(\", \")]))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Method: Doc2vec + Random Forest Classifier\n",
    "\n",
    "- Doc2vec Embeddings for definitions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing of definitions and creating the tagged documents for doc2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim pre-process (lowercase and tokenize)\n",
    "df = load_dataset()\n",
    "df['definition'] = df['definition'].apply(simple_preprocess)\n",
    "\n",
    "# Create tagged documents for each definition\n",
    "df['tagged_docs'] = df.apply(lambda x: TaggedDocument(x['definition'], [x['target']]), axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc2vec model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc2vec_model(train_tagged: list) -> Doc2Vec:\n",
    "    # Creating the Doc2Vec model\n",
    "    cores = multiprocessing.cpu_count()\n",
    "    model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, workers=cores, epochs=30)\n",
    "    model_dbow.build_vocab(train_tagged)\n",
    "\n",
    "    # Training the Doc2Vec model\n",
    "    model_dbow.train(utils.shuffle(train_tagged), total_examples=len(train_tagged), epochs=model_dbow.epochs)\n",
    "    \n",
    "    return model_dbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the final train and test vectors\n",
    "def vec_for_classifier(model: Doc2Vec, tagged_docs: list) -> tuple[tuple, tuple]:\n",
    "    # We have a list of tuples (target, vector of features) for each sentence and then we zip them to \n",
    "    # get a list of targets and a list of vectors\n",
    "    y, X = zip(*[(doc.tags[0], model.infer_vector(doc.words, epochs=20)) for doc in tagged_docs])\n",
    "    return X, y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Validation of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=3, estimator=RandomForestClassifier(),\n",
       "             param_grid={&#x27;criterion&#x27;: [&#x27;gini&#x27;, &#x27;entropy&#x27;], &#x27;max_depth&#x27;: [4, 5],\n",
       "                         &#x27;max_features&#x27;: [&#x27;sqrt&#x27;, &#x27;log2&#x27;],\n",
       "                         &#x27;n_estimators&#x27;: [200, 500]},\n",
       "             verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=3, estimator=RandomForestClassifier(),\n",
       "             param_grid={&#x27;criterion&#x27;: [&#x27;gini&#x27;, &#x27;entropy&#x27;], &#x27;max_depth&#x27;: [4, 5],\n",
       "                         &#x27;max_features&#x27;: [&#x27;sqrt&#x27;, &#x27;log2&#x27;],\n",
       "                         &#x27;n_estimators&#x27;: [200, 500]},\n",
       "             verbose=1)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=3, estimator=RandomForestClassifier(),\n",
       "             param_grid={'criterion': ['gini', 'entropy'], 'max_depth': [4, 5],\n",
       "                         'max_features': ['sqrt', 'log2'],\n",
       "                         'n_estimators': [200, 500]},\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding the best parameters for Random Forest Classifier\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "model_dbow = doc2vec_model(df['tagged_docs'])\n",
    "X, y = vec_for_classifier(model_dbow, df['tagged_docs'])\n",
    "\n",
    "param_grid = { \n",
    "    'n_estimators': [200, 500],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'max_depth' : [4,5],\n",
    "    'criterion' :['gini', 'entropy']\n",
    "}\n",
    "\n",
    "CV_rfc = GridSearchCV(estimator = rfc, param_grid = param_grid, cv = 3, refit = True, verbose = 1)\n",
    "CV_rfc.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy:  0.5742574257425742\n",
      "Mean F1 score:  0.5737887176618872\n"
     ]
    }
   ],
   "source": [
    "# 10-fold cross validation\n",
    "accuracy = []\n",
    "f1 = []\n",
    "\n",
    "for i in range(10):\n",
    "\n",
    "    # Split the dataset into train and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df['tagged_docs'], df['target'], test_size=0.2, random_state=random.randint(0, 1000))\n",
    "\n",
    "    model_dbow = doc2vec_model(X_train)\n",
    "    X_train, y_train = vec_for_classifier(model_dbow, X_train)\n",
    "    X_test, y_test = vec_for_classifier(model_dbow, X_test)\n",
    "\n",
    "    rfc = RandomForestClassifier(\n",
    "        n_estimators=CV_rfc.best_params_['n_estimators'],\n",
    "        max_features=CV_rfc.best_params_['max_features'],\n",
    "        max_depth=CV_rfc.best_params_['max_depth'],\n",
    "        criterion=CV_rfc.best_params_['criterion']\n",
    "    )\n",
    "    rfc.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = rfc.predict(X_test)\n",
    "    accuracy.append(accuracy_score(y_test, y_pred))\n",
    "    f1.append(f1_score(y_test, y_pred, average='weighted'))\n",
    "\n",
    "print(\"Mean Accuracy: \", np.mean(accuracy))\n",
    "print(\"Mean F1 score: \", np.mean(f1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Method: Sentence Bert + Support Vector Classifier\n",
    "\n",
    "- Sentence Bert Embeddings for definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reloading the dataset\n",
    "df = load_dataset()\n",
    "\n",
    "# Obtaining the Bert Embeddings\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# I compute the embeddings for the dataset\n",
    "bert_embeddings = model.encode(df['definition'], convert_to_tensor=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold Validation of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 32 candidates, totalling 320 fits\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=10, estimator=SVC(),\n",
       "             param_grid={&#x27;C&#x27;: [1, 10, 100, 1000],\n",
       "                         &#x27;gamma&#x27;: [1, 0.1, 0.001, 0.0001],\n",
       "                         &#x27;kernel&#x27;: [&#x27;linear&#x27;, &#x27;rbf&#x27;]},\n",
       "             verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=10, estimator=SVC(),\n",
       "             param_grid={&#x27;C&#x27;: [1, 10, 100, 1000],\n",
       "                         &#x27;gamma&#x27;: [1, 0.1, 0.001, 0.0001],\n",
       "                         &#x27;kernel&#x27;: [&#x27;linear&#x27;, &#x27;rbf&#x27;]},\n",
       "             verbose=1)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=10, estimator=SVC(),\n",
       "             param_grid={'C': [1, 10, 100, 1000],\n",
       "                         'gamma': [1, 0.1, 0.001, 0.0001],\n",
       "                         'kernel': ['linear', 'rbf']},\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding the best parameters for SVC\n",
    "svc = SVC()\n",
    "\n",
    "param_grid = {\n",
    "    'C': [1, 10, 100, 1000],\n",
    "    'gamma': [1, 0.1, 0.001, 0.0001],\n",
    "    'kernel': ['linear', 'rbf']\n",
    "}\n",
    "\n",
    "CV_svc = GridSearchCV(estimator = svc, param_grid = param_grid, cv = 10, refit = True, verbose = 1)\n",
    "CV_svc.fit(bert_embeddings, df['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy:  0.7019801980198019\n",
      "Mean F1 score:  0.7010795224798815\n"
     ]
    }
   ],
   "source": [
    "# 10-fold cross validation\n",
    "accuracy = []\n",
    "f1 = []\n",
    "\n",
    "for i in range(10):\n",
    "    # Splitting the dataset in train and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(bert_embeddings, df['target'], test_size=0.2, random_state=random.randint(0, 1000))\n",
    "\n",
    "    # Training the model\n",
    "    svc = SVC(\n",
    "        C = CV_svc.best_params_['C'],\n",
    "        gamma = CV_svc.best_params_['gamma'],\n",
    "        kernel = CV_svc.best_params_['kernel']\n",
    "    )\n",
    "    svc.fit(X_train, y_train)\n",
    "\n",
    "    # Predicting the test set\n",
    "    y_pred = svc.predict(X_test)\n",
    "    accuracy.append(accuracy_score(y_test, y_pred))\n",
    "    f1.append(f1_score(y_test, y_pred, average='weighted'))\n",
    "\n",
    "print(\"Mean Accuracy: \", np.mean(accuracy))\n",
    "print(\"Mean F1 score: \", np.mean(f1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third Method: Glove embeddings + DNN\n",
    "\n",
    "- Embeddings of the synonyms for each synset, that they are the lemmas of the synset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation of the Tokenizer and the embedding matrix for the DNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the GloVe Word Embeddings\n",
    "embeddings_index = dict()\n",
    "with open('data/glove.6B.100d.txt', encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype=\"float32\")\n",
    "        embeddings_index[word] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer(training_set: pd.Series, test_set: pd.Series, df: pd.DataFrame) -> tuple[np.ndarray, np.ndarray, Tokenizer, int, int]:\n",
    "    # Prepare tokenizer: the Tokenizer converts the words into integers\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(training_set)\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "    # Integer encode the documents\n",
    "    encoded_docs = tokenizer.texts_to_sequences(training_set)\n",
    "\n",
    "    # Pad documents to a max length of the word with the most synonyms\n",
    "    # for having a fixed length vectors\n",
    "    max_length = max([len(s.split()) for s in df['synonyms']])\n",
    "    padded_train = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "\n",
    "    # Tokenize the test set\n",
    "    X_test = tokenizer.texts_to_sequences(test_set)\n",
    "    padded_test = pad_sequences(X_test, maxlen=max_length, padding='post')\n",
    "\n",
    "    return padded_train, padded_test, tokenizer, vocab_size, max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Embedding matrix: a matrix of one embedding for each unique word in the training dataset\n",
    "# The result is a matrix of weights only for words we will see during training, with the embedding\n",
    "# weights obtained from the GloVe embedding\n",
    "\n",
    "def get_embedding_matrix(tokenizer: Tokenizer, vocab_size: int) -> np.ndarray:\n",
    "\tembedding_matrix = np.zeros((vocab_size, 100))\n",
    "\tfor word, i in tokenizer.word_index.items():\n",
    "\t\tembedding_vector = embeddings_index.get(word)\n",
    "\t\tif embedding_vector is not None:\n",
    "\t\t\tembedding_matrix[i] = embedding_vector\n",
    "\t\n",
    "\treturn embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(padded_train: np.ndarray, embedding_matrix: np.ndarray, vocab_size: int, target: pd.Series, max_length: int) -> Sequential:\n",
    "    # Defining NN model for classification\n",
    "    model = Sequential()\n",
    "\n",
    "    # Embedding layer: can be seeded with the GloVe word embedding weights\n",
    "    model.add(Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_length, trainable=False))\n",
    "\n",
    "    # Flattens the input\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # Deeply connected neural network layer\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "    # Fit the model\n",
    "    model.fit(padded_train, target, epochs=50, verbose=0)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Validation of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 999us/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "Mean Accuracy:  0.6594059405940593\n",
      "Mean F1 score:  0.6579329778365641\n"
     ]
    }
   ],
   "source": [
    "# 10-fold cross validation\n",
    "accuracy = []\n",
    "f1 = []\n",
    "\n",
    "for i in range(10):\n",
    "    # Splitting the dataset in train and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df['synonyms'], df['target'], test_size=0.2, random_state=random.randint(0, 1000))\n",
    "    \n",
    "    X_padded, y_padded, tokenizer, vocab_size, max_length = get_tokenizer(X_train, X_test, df)\n",
    "    embeddings_matrix = get_embedding_matrix(tokenizer, vocab_size)\n",
    "    model = fit_model(X_padded, embeddings_matrix, vocab_size, y_train, max_length)\n",
    "\n",
    "    # Predicting the test set\n",
    "    y_pred = model.predict(y_padded).round()\n",
    "    accuracy.append(accuracy_score(y_test, y_pred))\n",
    "    f1.append(f1_score(y_test, y_pred, average='weighted'))\n",
    "\n",
    "print(\"Mean Accuracy: \", np.mean(accuracy))\n",
    "print(\"Mean F1 score: \", np.mean(f1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fourth Method: Sentence Bert + Glove + RF\n",
    "\n",
    "- Sentence Bert embeddings for the definitions\n",
    "- Glove embeddings for the synonyms\n",
    "- RF model for the classification (because has a better performance than the SVM or DNN)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading dataset, GloVe embeddings and defining two support functions for getting the embeddings of the synonyms and obtaining the final embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reloading the dataset and GloVe embeddings\n",
    "df = load_dataset()\n",
    "\n",
    "# Loading the GloVe Word Embeddings\n",
    "embeddings_index = dict()\n",
    "with open('data/glove.6B.100d.txt', encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype=\"float32\")\n",
    "        embeddings_index[word] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each synonyms of a synonym group, i compute the sum embedding\n",
    "\n",
    "def get_synonyms_embeddings(df: pd.DataFrame) -> list:\n",
    "    embeddings_synonyms = []\n",
    "\n",
    "    for synonym in df['synonyms']:\n",
    "        synonyms = synonym.split(', ')\n",
    "        avg_embedding = []\n",
    "        for word in synonyms:\n",
    "            if word in embeddings_index:\n",
    "                avg_embedding.append(embeddings_index[word])\n",
    "            else:\n",
    "                avg_embedding.append(np.zeros(100))\n",
    "        embeddings_synonyms.append(np.sum(avg_embedding, axis=0))\n",
    "\n",
    "    return embeddings_synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_embeddings(df: pd.DataFrame, bert_embeddings_definition: list, glove_embeddings_synonyms: list) -> list:\n",
    "\n",
    "    # Creating a single vector with the embeddings of the definition, the synonyms, the timeDiffs and the isHard\n",
    "    final_embeddings = []\n",
    "    for i in range(len(df)):\n",
    "        emb = []\n",
    "        for embedding in bert_embeddings_definition[i].numpy().tolist():\n",
    "            emb.append(embedding)\n",
    "        for embedding in glove_embeddings_synonyms[i]:\n",
    "            emb.append(embedding)\n",
    "        emb.append(df['timeDiffs'].iloc[i])\n",
    "        emb.append(df['isHard'].iloc[i])\n",
    "        final_embeddings.append(emb)\n",
    "\n",
    "    return final_embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the GloVe embeddings for the synonyms, the Sentence Bert embeddings for the definitions and I append them together in a single matrix with also the \"timeDiffs\" and the \"isHard\" features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I compute the embeddings for the synonyms\n",
    "glove_embeddings_synonyms = get_synonyms_embeddings(df)\n",
    "\n",
    "# Obtaining the Bert Embeddings\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# I compute the embeddings for the dataset\n",
    "bert_embeddings_definition = model.encode(df['definition'].to_list(), convert_to_tensor=True)\n",
    "\n",
    "# I compute the final embeddings\n",
    "final_embeddings = get_final_embeddings(df, bert_embeddings_definition, glove_embeddings_synonyms)\n",
    "\n",
    "# I add a column to the dataset with the embeddings\n",
    "df['embeddings'] = final_embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Validation of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=3, estimator=RandomForestClassifier(),\n",
       "             param_grid={&#x27;criterion&#x27;: [&#x27;gini&#x27;, &#x27;entropy&#x27;], &#x27;max_depth&#x27;: [4, 5],\n",
       "                         &#x27;max_features&#x27;: [&#x27;sqrt&#x27;, &#x27;log2&#x27;],\n",
       "                         &#x27;n_estimators&#x27;: [200, 500]},\n",
       "             verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=3, estimator=RandomForestClassifier(),\n",
       "             param_grid={&#x27;criterion&#x27;: [&#x27;gini&#x27;, &#x27;entropy&#x27;], &#x27;max_depth&#x27;: [4, 5],\n",
       "                         &#x27;max_features&#x27;: [&#x27;sqrt&#x27;, &#x27;log2&#x27;],\n",
       "                         &#x27;n_estimators&#x27;: [200, 500]},\n",
       "             verbose=1)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" ><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\" ><label for=\"sk-estimator-id-18\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=3, estimator=RandomForestClassifier(),\n",
       "             param_grid={'criterion': ['gini', 'entropy'], 'max_depth': [4, 5],\n",
       "                         'max_features': ['sqrt', 'log2'],\n",
       "                         'n_estimators': [200, 500]},\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding the best parameters for Random Forest Classifier\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "param_grid = { \n",
    "    'n_estimators': [200, 500],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'max_depth' : [4,5],\n",
    "    'criterion' :['gini', 'entropy']\n",
    "}\n",
    "\n",
    "CV_rfc = GridSearchCV(estimator = rfc, param_grid = param_grid, cv = 3, refit = True, verbose = 1)\n",
    "CV_rfc.fit(df['embeddings'].tolist(), df['target'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy:  0.8465346534653465\n",
      "Mean F1 score:  0.8454603245599493\n"
     ]
    }
   ],
   "source": [
    "# 10-fold cross validation\n",
    "accuracy = []\n",
    "f1 = []\n",
    "\n",
    "for i in range(10):\n",
    "    # Splitting the dataset in train and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df['embeddings'].tolist(), df['target'].tolist(), test_size=0.2, random_state=random.randint(0, 1000))\n",
    "    \n",
    "    rfc = RandomForestClassifier(\n",
    "        n_estimators=CV_rfc.best_params_['n_estimators'],\n",
    "        max_features=CV_rfc.best_params_['max_features'],\n",
    "        max_depth=CV_rfc.best_params_['max_depth'],\n",
    "        criterion=CV_rfc.best_params_['criterion']\n",
    "    )\n",
    "    rfc.fit(X_train, y_train)\n",
    "\n",
    "    # Predicting the test set\n",
    "    y_pred = rfc.predict(X_test)\n",
    "    accuracy.append(accuracy_score(y_test, y_pred))\n",
    "    f1.append(f1_score(y_test, y_pred, average='weighted'))  \n",
    "\n",
    "print(\"Mean Accuracy: \", np.mean(accuracy))\n",
    "print(\"Mean F1 score: \", np.mean(f1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
