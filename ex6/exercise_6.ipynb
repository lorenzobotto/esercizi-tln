{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6 - Part C"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import multiprocessing\n",
    "import random\n",
    "from sklearn import utils\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.utils import simple_preprocess\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the structure of pandas dataframe\n",
    "df = pd.DataFrame(columns=[\"synset\", \"synonyms\", \"definition\", \"target\"])\n",
    "\n",
    "for filename in os.listdir(\"data/\"):\n",
    "    with open(os.path.join(\"data/\", filename), 'r') as f:\n",
    "        if filename == \"1.json\":\n",
    "            data = json.loads(f.read())\n",
    "            synsets = data[\"dataset\"]\n",
    "            for index, synset in enumerate(synsets):\n",
    "                df.loc[len(df)] = [\n",
    "                    synset.split(\":\")[0],\n",
    "                    synset.split(\":\")[1].split(\"|\")[0].strip(),\n",
    "                    synset.split(\":\")[2].strip(),\n",
    "                    data[\"answers\"][index]\n",
    "                ]\n",
    "        else:\n",
    "            data = json.loads(f.read())\n",
    "            answers = data[\"answers\"]\n",
    "            for index, answer in enumerate(answers):\n",
    "                df.at[index, 'target'] = df.iloc[index]['target'] + \", \" + answer\n",
    "\n",
    "# Produce an unique target label that is the common one\n",
    "df['target'] = df['target'].apply(lambda x: max(set(x.split(\", \")), key=x.split(\", \").count))\n",
    "df['target'] = df['target'].map({'basic': 0, 'advanced': 1})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Method: Using doc2vec to create a vector representation of each definition and then using a RandomForestClassifier to classify the definitions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing of definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim pre-process (tokenize, remove stopwords, tokenize)\n",
    "df['definition'] = df['definition'].apply(simple_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagged_documents(df: pd.DataFrame) -> list:\n",
    "    # Splitting the dataset\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df['definition'], df['target'], test_size=0.2, random_state=random.randint(0, 1000))\n",
    "\n",
    "    # Creating the tagged documents for the Doc2Vec model\n",
    "    train_tagged = []\n",
    "    for index, sentence in enumerate(X_train):\n",
    "        train_tagged.append(TaggedDocument(sentence, [y_train.iloc[index]]))\n",
    "\n",
    "    test_tagged = []\n",
    "    for index, sentence in enumerate(X_test):\n",
    "        test_tagged.append(TaggedDocument(sentence, [y_test.iloc[index]]))\n",
    "    \n",
    "    return train_tagged, test_tagged"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc2vec model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc2vec_model(train_tagged: list) -> Doc2Vec:\n",
    "    # Creating the Doc2Vec model\n",
    "    cores = multiprocessing.cpu_count()\n",
    "    model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, workers=cores, epochs=30)\n",
    "    model_dbow.build_vocab([x for x in train_tagged])\n",
    "\n",
    "    # Training the Doc2Vec model\n",
    "    model_dbow.train(utils.shuffle([x for x in train_tagged]), total_examples=len(train_tagged), epochs=model_dbow.epochs)\n",
    "    \n",
    "    return model_dbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the final train and test vectors\n",
    "def vec_for_classifier(model, tagged_docs):\n",
    "    sents = tagged_docs\n",
    "    y, X = zip(*[(doc.tags[0], model.infer_vector(doc.words, epochs=20)) for doc in sents])\n",
    "    return y, X"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the doc2vec model with RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.5841584158415841\n",
      "F1 score: 0.5776919797242883\n"
     ]
    }
   ],
   "source": [
    "# Training the Random Forest Model\n",
    "train_tagged, test_tagged = tagged_documents(df)\n",
    "model_dbow = doc2vec_model(train_tagged)\n",
    "y_train, X_train = vec_for_classifier(model_dbow, train_tagged)\n",
    "y_test, X_test = vec_for_classifier(model_dbow, test_tagged)\n",
    "\n",
    "rfc = RandomForestClassifier(max_depth=4)\n",
    "rfc.fit(X_train, y_train)\n",
    "y_pred = rfc.predict(X_test)\n",
    "\n",
    "print('Accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Validation of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy:  0.6198019801980197\n",
      "Mean F1 score:  0.620067424332132\n"
     ]
    }
   ],
   "source": [
    "# 10-fold cross validation: i retrain the doc2vec model and the random forest model 10 times\n",
    "accuracy = []\n",
    "f1 = []\n",
    "\n",
    "for i in range(10):\n",
    "    train_tagged, test_tagged = tagged_documents(df)\n",
    "    model_dbow = doc2vec_model(train_tagged)\n",
    "    y_train, X_train = vec_for_classifier(model_dbow, train_tagged)\n",
    "    y_test, X_test = vec_for_classifier(model_dbow, test_tagged)\n",
    "\n",
    "    rfc = RandomForestClassifier(max_depth=4)\n",
    "    rfc.fit(X_train, y_train)\n",
    "    y_pred = rfc.predict(X_test)\n",
    "    accuracy.append(accuracy_score(y_test, y_pred))\n",
    "    f1.append(f1_score(y_test, y_pred, average='weighted'))\n",
    "\n",
    "print(\"Mean Accuracy: \", np.mean(accuracy))\n",
    "print(\"Mean F1 score: \", np.mean(f1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
